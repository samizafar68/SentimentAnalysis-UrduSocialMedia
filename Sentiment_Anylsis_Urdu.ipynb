{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>๐คฃ๐๐ ู ูู ุฏ ูุฑ ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ  ฺฉูุฌ ู...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ฺู ููุงููฺบ ูฺบ ฺฉฺพุงูุง ุณุฑู ฺฉุฑ ฺฺู ฺุงฺ ููฺบ ุฏุณุฏ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ฺฉุงูุฑุงู ุฎุงู ุขูพฺฉ ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ฺฏุฆ ุงูพ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ูฺบ ูพุงุฆู ๐</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` ูุฑุงุฏ ุนู ุดุง ฺฉ ุจฺพุณ ูฺบ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ูุงุจู ุงุนุชุจุงุฑ  ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ ูุช ฺบ ๐๐ฅ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ุงูุณุงฺบ ฺฉู ุชฺพฺฉุง ุฏุชุง  ุณูฺูฺบ ฺฉุง ุณูุฑ ุจฺพ ... ๐๐ฅ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู๐๐</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ุงุฑ ูฺุงุฑ ููุง ููุฏุง  ุงุณ ุขุฑ ูฺฏุง ูุง ๐๐ ุช...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td> ุณูุฌฺพุช ฺบ ุณุงุฑุง ูพุงฺฉุณุชุงู ุจูููู ฺพ ๐๐๐</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  is_sarcastic  \\\n",
       "0  ๐คฃ๐๐ ู ูู ุฏ ูุฑ ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ  ฺฉูุฌ ู...           1.0   \n",
       "1  ฺู ููุงููฺบ ูฺบ ฺฉฺพุงูุง ุณุฑู ฺฉุฑ ฺฺู ฺุงฺ ููฺบ ุฏุณุฏ...           1.0   \n",
       "2  ฺฉุงูุฑุงู ุฎุงู ุขูพฺฉ ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ฺฏุฆ ุงูพ...           0.0   \n",
       "3                                       ูฺบ ูพุงุฆู ๐           0.0   \n",
       "4   `` ูุฑุงุฏ ุนู ุดุง ฺฉ ุจฺพุณ ูฺบ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ...           1.0   \n",
       "5        ูุงุจู ุงุนุชุจุงุฑ  ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ ูุช ฺบ ๐๐ฅ           1.0   \n",
       "6      ุงูุณุงฺบ ฺฉู ุชฺพฺฉุง ุฏุชุง  ุณูฺูฺบ ฺฉุง ุณูุฑ ุจฺพ ... ๐๐ฅ           0.0   \n",
       "7                              ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู๐๐           0.0   \n",
       "8  ุงุฑ ูฺุงุฑ ููุง ููุฏุง  ุงุณ ุขุฑ ูฺฏุง ูุง ๐๐ ุช...           1.0   \n",
       "9            ุณูุฌฺพุช ฺบ ุณุงุฑุง ูพุงฺฉุณุชุงู ุจูููู ฺพ ๐๐๐           1.0   \n",
       "\n",
       "   Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5 Unnamed: 6  Unnamed: 7  \n",
       "0         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "1         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "2         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "3         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "4         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "5         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "6         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "7         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "8         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "9         NaN         NaN         NaN         NaN        NaN         NaN  "
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load data\n",
    "file_path = 'urdu_sarcastic_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first 10 lines of Datset\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>Stopword_removal_cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>๐คฃ๐๐ ู ูู ุฏ ูุฑ ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ  ฺฉูุฌ ู...</td>\n",
       "      <td>๐คฃ๐๐ ูู ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ ฺฉูุฌ ูฺบ ๐๐๐๐คฃ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ฺู ููุงููฺบ ูฺบ ฺฉฺพุงูุง ุณุฑู ฺฉุฑ ฺฺู ฺุงฺ ููฺบ ุฏุณุฏ...</td>\n",
       "      <td>ููุงููฺบ ฺฉฺพุงูุง ฺฺู ฺุงฺ ุฏุณุฏ ูฺบ๐๐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ฺฉุงูุฑุงู ุฎุงู ุขูพฺฉ ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ฺฏุฆ ุงูพ...</td>\n",
       "      <td>ฺฉุงูุฑุงู ุฎุงู ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุดู ฺฉุฑุฏ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ูฺบ ูพุงุฆู ๐</td>\n",
       "      <td>ูฺบ ูพุงุฆู ๐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` ูุฑุงุฏ ุนู ุดุง ฺฉ ุจฺพุณ ูฺบ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ...</td>\n",
       "      <td>`` ูุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ '' ุญุงูุฏ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ูุงุจู ุงุนุชุจุงุฑ  ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ ูุช ฺบ ๐๐ฅ</td>\n",
       "      <td>ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ ๐๐ฅ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ุงูุณุงฺบ ฺฉู ุชฺพฺฉุง ุฏุชุง  ุณูฺูฺบ ฺฉุง ุณูุฑ ุจฺพ ... ๐๐ฅ</td>\n",
       "      <td>ุงูุณุงฺบ ุชฺพฺฉุง ุณูฺูฺบ ุณูุฑ ... ๐๐ฅ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู๐๐</td>\n",
       "      <td>ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู๐๐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ุงุฑ ูฺุงุฑ ููุง ููุฏุง  ุงุณ ุขุฑ ูฺฏุง ูุง ๐๐ ุช...</td>\n",
       "      <td>ุงุฑ ูฺุงุฑ ููุง ููุฏุง ุขุฑ ูฺฏุง ูุง ๐๐ ุชุณ ูพฺฉ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td> ุณูุฌฺพุช ฺบ ุณุงุฑุง ูพุงฺฉุณุชุงู ุจูููู ฺพ ๐๐๐</td>\n",
       "      <td>ุณูุฌฺพุช ูพุงฺฉุณุชุงู ุจูููู ๐๐๐</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  \\\n",
       "0  ๐คฃ๐๐ ู ูู ุฏ ูุฑ ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ  ฺฉูุฌ ู...   \n",
       "1  ฺู ููุงููฺบ ูฺบ ฺฉฺพุงูุง ุณุฑู ฺฉุฑ ฺฺู ฺุงฺ ููฺบ ุฏุณุฏ...   \n",
       "2  ฺฉุงูุฑุงู ุฎุงู ุขูพฺฉ ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ฺฏุฆ ุงูพ...   \n",
       "3                                       ูฺบ ูพุงุฆู ๐   \n",
       "4   `` ูุฑุงุฏ ุนู ุดุง ฺฉ ุจฺพุณ ูฺบ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ...   \n",
       "5        ูุงุจู ุงุนุชุจุงุฑ  ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ ูุช ฺบ ๐๐ฅ   \n",
       "6      ุงูุณุงฺบ ฺฉู ุชฺพฺฉุง ุฏุชุง  ุณูฺูฺบ ฺฉุง ุณูุฑ ุจฺพ ... ๐๐ฅ   \n",
       "7                              ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู๐๐   \n",
       "8  ุงุฑ ูฺุงุฑ ููุง ููุฏุง  ุงุณ ุขุฑ ูฺฏุง ูุง ๐๐ ุช...   \n",
       "9            ุณูุฌฺพุช ฺบ ุณุงุฑุง ูพุงฺฉุณุชุงู ุจูููู ฺพ ๐๐๐   \n",
       "\n",
       "                       Stopword_removal_cleaned_text  \n",
       "0            ๐คฃ๐๐ ูู ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ ฺฉูุฌ ูฺบ ๐๐๐๐คฃ  \n",
       "1                 ููุงููฺบ ฺฉฺพุงูุง ฺฺู ฺุงฺ ุฏุณุฏ ูฺบ๐๐  \n",
       "2  ฺฉุงูุฑุงู ุฎุงู ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุดู ฺฉุฑุฏ...  \n",
       "3                                       ูฺบ ูพุงุฆู ๐  \n",
       "4  `` ูุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ '' ุญุงูุฏ...  \n",
       "5                    ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ ๐๐ฅ  \n",
       "6                        ุงูุณุงฺบ ุชฺพฺฉุง ุณูฺูฺบ ุณูุฑ ... ๐๐ฅ  \n",
       "7                              ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู๐๐  \n",
       "8  ุงุฑ ูฺุงุฑ ููุง ููุฏุง ุขุฑ ูฺฏุง ูุง ๐๐ ุชุณ ูพฺฉ...  \n",
       "9                          ุณูุฌฺพุช ูพุงฺฉุณุชุงู ุจูููู ๐๐๐  "
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "exception_stopwords = [\n",
    "    'ูฺบ',  # No, Not\n",
    "    'ุจุฑุง',   # Bad\n",
    "    'ุงฺฺพุง',  # Good\n",
    "    'ุจุชุฑ',  # Better\n",
    "    'ุฎุฑุงุจ',  # Bad/Destroyed\n",
    "    'ุบูุท',   # Wrong\n",
    "    'ุณฺ',    # True\n",
    "    'ุฌฺพููน',  # False/Lie\n",
    "    'ุฎูุด',   # Happy\n",
    "    'ุงุฏุงุณ',  # Sad\n",
    "    'ูุญุจุช',  # Love\n",
    "    'ููุฑุช',  # Hate\n",
    "    'ุฎูู',   # Fear\n",
    "    'ุฎูุจุตูุฑุช', # Beautiful\n",
    "    'ุจุฏุตูุฑุช', # Ugly\n",
    "    'ุฒุงุฏ',  # More\n",
    "    'ฺฉู',     # Less\n",
    "    'ุจุช',    # Very\n",
    "    'ุงูุชุงุฆ', # Extremely\n",
    "    'ุจุฑุง',   # Wrongly\n",
    "    'ูุนููู', # Slight\n",
    "    'ุญููุช',  # Reality\n",
    "    'ุจฺฉุงุฑ',  # Useless\n",
    "    'ูพุงุฑ',   # Love\n",
    "    'ุธุงูู',   # Cruel\n",
    "    'ูฺฉ',    # Good/Pious\n",
    "    'ุบุฑุจ',   # Poor\n",
    "    'ุงูุฑ',   # Rich\n",
    "    'ุณุณุช',    # Slow/Lazy\n",
    "    'ุชุฒ',    # Fast\n",
    "]\n",
    "\n",
    "# Function to remove stopwords, excluding exception words\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        words = text.split()  # Tokenize the text\n",
    "        # Only include words that are not in urdu_stopwords, or are in exception_stopwords\n",
    "        cleaned_words = [word for word in words if word not in urdu_stopwords or word in exception_stopwords]\n",
    "        return ' '.join(cleaned_words)  # Join the words back into a string\n",
    "    return '' \n",
    "urdu_stopwords = {\n",
    "    \"ู\", \"ูฺบ\", \"ูุฑ\", \"ูุฑุง\", \"ูุฑ\", \"ุงุณ\", \"ุงุณฺฉุง\", \"ุงุณฺฉ\", \"ุงุณฺฉ\", \"ฺฏุง\",\n",
    "    \"ฺฏ\", \"ฺฏ\", \"ู\", \"ูฺบ\", \"ูุงุฑ\", \"ูุงุฑุง\", \"ุงูุฑ\", \"ฺฉุง\", \"ฺฉูฺบ\", \"ู\",\n",
    "    \"ูพุฑ\", \"\", \"ฺฉ\", \"ุณ\", \"ฺฉุง\", \"ฺฉ\", \"ุชฺพุง\", \"ุชฺพ\", \"ุชฺพ\", \"ู\",\n",
    "    \"ุงฺฉ\", \"ุชู\", \"ุชูุงุฑ\", \"ุชูุงุฑุง\", \"ุชูุงุฑ\", \"ุชูฺบ\", \"ุขูพ\", \"ุขูพฺฉุง\", \"ุขูพฺฉ\", \"ุขูพฺฉ\",\n",
    "    \"ฺบ\", \"ุงุณ\", \"ุงู\", \"ุจฺพ\", \"ุง\", \"ูฺบ\", \"ฺฉฺฺพ\", \"ุฑ\", \"ูฺฏุฑ\", \"ุฌู\",\n",
    "    \"ุงูฺฉุง\", \"ุงูฺฉ\", \"ุงูฺฉ\", \"ุงูพูุง\", \"ุงูพู\", \"ุงูพู\", \"ุงุจฺพ\", \"ุจุช\", \"ุตุฑู\", \"ูฺฉู\",\n",
    "    \"ูพฺพุฑ\", \"\", \"ูุงฺบ\", \"ฺฉุงฺบ\", \"ฺฉุณ\", \"ฺฉุจ\", \"ฺฉุณ\", \"ฺุง\", \"ุขุฌ\", \"ฺฉู\",\n",
    "    \"ูพุฑ\", \"ุชฺฉ\", \"ุงฺบ\", \"ูุงฺบ\", \"ุงฺฏุฑ\", \"ฺฉ\", \"ููุณู\", \"ฺฉูฺบ\", \"ุจฺพ\", \"ู ุณฺฉุชุง\",\n",
    "    \"ุจุช ุฒุงุฏ\", \"ุงุณ ู\", \"ุขุชุง\", \"ฺฏุง\", \"ุฏุง\", \"ฺูุง\", \"ุจูุง\", \"ฺฉุง\", \"ุฏฺฉฺพุง\", \"ูุฆ\",\n",
    "    \"ุขุชุง\", \"ฺุงุชุง\", \"ูุชุง\", \"ุฑุชุง\", \"ฺุงุชุง\", \"ูพุงุชุง\", \"ูพูฺุชุง\", \"ุฏฺฉฺพุชุง\", \"ูุชุง\", \"ฺฉุฑุชุง\",\n",
    "    \"ฺูู\", \"ูุฆ\", \"ฺฏุฆ\", \"ฺฏุฆ\", \"ฺูุง ฺฏุง\", \"ุขุฆ\", \"ฺฉูุดุด\", \"ฺฉ\", \"ฺฉุฑู\", \"ฺฉุฑูุง\",\n",
    "    \"ุงุฏ\", \"ฺุงุช\", \"ุชฺพุง\", \"ุชฺพุง\", \"ุฑ\", \"ุฑุง\", \"ฺฉุฑ ุฑ\", \"ฺฉุฑ ุฑุง\", \"ุขุฆ\", \"ุขุชุง\",\n",
    "    'ุจฺพุฑ','', 'ูฺบ', 'ุงูุฑ', 'ฺฉุง', '', 'ฺฉ', 'ฺฉ', '', 'ุชฺพุง', 'ุชฺพ', 'ูฺบ', 'ุขูพ', 'ู', \n",
    "    'ุขฺบ','ููฺบ','ฺู','ฺฉุง', 'ูพุฑ', 'ฺฉฺฺพ', 'ู', 'ูฺบ', 'ฺฉ', 'ูฺบ', 'ุจฺพ', 'ุฌู', 'ุชู'  # Add 'ุชู' here\n",
    "\"ฺฉุฑุฆ\",\"ุฑฺฉฺพ\",\"ุฑฺฏุง\",\"ุช\",\"ุณ\",\"ู\",\"ุงูพ\",\"ุณุงฺ\",\"ุงู\",\"ฺฉุฑู\",\"ุณุงุฑุง\",\"ฺพ\",\"\",\"ุฏุชุง\",\"ฺฉู\",\"ูุช\",\"\",\"ุขูพ\",\"ฺฉุฑุช\",\"ุณ\",\"ุณุจ\",\"ุฏ\",\"ุณุฑู\",\"ฺฉุฑ\",\"ุขุฆ\", \"ุขุชุง\", \"ูพูฺุชุง\", \"ุขุชุง\", \"ฺุงุชุง\", \"ุขุชุง\",\n",
    "}\n",
    "\n",
    "# By Peform Stopword Reoving to the 'urdu_text' column\n",
    "df['Stopword_removal_cleaned_text'] = df['urdu_text'].apply(remove_stopwords)\n",
    "\n",
    "# Display the first 10 lines after removing Stopwords\n",
    "df[['urdu_text','Stopword_removal_cleaned_text']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>Stopword_removal_cleaned_text</th>\n",
       "      <th>Emoji_Punctuation_url_Stopword_removal_cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>๐คฃ๐๐ ู ูู ุฏ ูุฑ ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ  ฺฉูุฌ ู...</td>\n",
       "      <td>๐คฃ๐๐ ูู ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ ฺฉูุฌ ูฺบ ๐๐๐๐คฃ</td>\n",
       "      <td>ูู ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ ฺฉูุฌ ูฺบ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ฺู ููุงููฺบ ูฺบ ฺฉฺพุงูุง ุณุฑู ฺฉุฑ ฺฺู ฺุงฺ ููฺบ ุฏุณุฏ...</td>\n",
       "      <td>ููุงููฺบ ฺฉฺพุงูุง ฺฺู ฺุงฺ ุฏุณุฏ ูฺบ๐๐</td>\n",
       "      <td>ููุงููฺบ ฺฉฺพุงูุง ฺฺู ฺุงฺ ุฏุณุฏ ูฺบ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ฺฉุงูุฑุงู ุฎุงู ุขูพฺฉ ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ฺฏุฆ ุงูพ...</td>\n",
       "      <td>ฺฉุงูุฑุงู ุฎุงู ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุดู ฺฉุฑุฏ...</td>\n",
       "      <td>ฺฉุงูุฑุงู ุฎุงู ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุดู ฺฉุฑุฏ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` ูุฑุงุฏ ุนู ุดุง ฺฉ ุจฺพุณ ูฺบ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ...</td>\n",
       "      <td>`` ูุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ '' ุญุงูุฏ...</td>\n",
       "      <td>ูุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ ุญุงูุฏ ูุฑ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ูุงุจู ุงุนุชุจุงุฑ  ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ ูุช ฺบ ๐๐ฅ</td>\n",
       "      <td>ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ ๐๐ฅ</td>\n",
       "      <td>ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ุงูุณุงฺบ ฺฉู ุชฺพฺฉุง ุฏุชุง  ุณูฺูฺบ ฺฉุง ุณูุฑ ุจฺพ ... ๐๐ฅ</td>\n",
       "      <td>ุงูุณุงฺบ ุชฺพฺฉุง ุณูฺูฺบ ุณูุฑ ... ๐๐ฅ</td>\n",
       "      <td>ุงูุณุงฺบ ุชฺพฺฉุง ุณูฺูฺบ ุณูุฑ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู๐๐</td>\n",
       "      <td>ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู๐๐</td>\n",
       "      <td>ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ุงุฑ ูฺุงุฑ ููุง ููุฏุง  ุงุณ ุขุฑ ูฺฏุง ูุง ๐๐ ุช...</td>\n",
       "      <td>ุงุฑ ูฺุงุฑ ููุง ููุฏุง ุขุฑ ูฺฏุง ูุง ๐๐ ุชุณ ูพฺฉ...</td>\n",
       "      <td>ุงุฑ ูฺุงุฑ ููุง ููุฏุง ุขุฑ ูฺฏุง ูุง  ุชุณ ูพฺฉ ู...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td> ุณูุฌฺพุช ฺบ ุณุงุฑุง ูพุงฺฉุณุชุงู ุจูููู ฺพ ๐๐๐</td>\n",
       "      <td>ุณูุฌฺพุช ูพุงฺฉุณุชุงู ุจูููู ๐๐๐</td>\n",
       "      <td>ุณูุฌฺพุช ูพุงฺฉุณุชุงู ุจูููู</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ุชุณ ูฺุงฺบู ฺฉุฑูุงู ุณุงฺ ฺฉ ๐๐๐</td>\n",
       "      <td>ุชุณ ูฺุงฺบู ฺฉุฑูุงู ๐๐๐</td>\n",
       "      <td>ุชุณ ูฺุงฺบู ฺฉุฑูุงู</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            urdu_text  \\\n",
       "0   ๐คฃ๐๐ ู ูู ุฏ ูุฑ ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ  ฺฉูุฌ ู...   \n",
       "1   ฺู ููุงููฺบ ูฺบ ฺฉฺพุงูุง ุณุฑู ฺฉุฑ ฺฺู ฺุงฺ ููฺบ ุฏุณุฏ...   \n",
       "2   ฺฉุงูุฑุงู ุฎุงู ุขูพฺฉ ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ฺฏุฆ ุงูพ...   \n",
       "4    `` ูุฑุงุฏ ุนู ุดุง ฺฉ ุจฺพุณ ูฺบ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ...   \n",
       "5         ูุงุจู ุงุนุชุจุงุฑ  ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ ูุช ฺบ ๐๐ฅ   \n",
       "6       ุงูุณุงฺบ ฺฉู ุชฺพฺฉุง ุฏุชุง  ุณูฺูฺบ ฺฉุง ุณูุฑ ุจฺพ ... ๐๐ฅ   \n",
       "7                               ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู๐๐   \n",
       "8   ุงุฑ ูฺุงุฑ ููุง ููุฏุง  ุงุณ ุขุฑ ูฺฏุง ูุง ๐๐ ุช...   \n",
       "9             ุณูุฌฺพุช ฺบ ุณุงุฑุง ูพุงฺฉุณุชุงู ุจูููู ฺพ ๐๐๐   \n",
       "10                      ุชุณ ูฺุงฺบู ฺฉุฑูุงู ุณุงฺ ฺฉ ๐๐๐   \n",
       "\n",
       "                        Stopword_removal_cleaned_text  \\\n",
       "0             ๐คฃ๐๐ ูู ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ ฺฉูุฌ ูฺบ ๐๐๐๐คฃ   \n",
       "1                  ููุงููฺบ ฺฉฺพุงูุง ฺฺู ฺุงฺ ุฏุณุฏ ูฺบ๐๐   \n",
       "2   ฺฉุงูุฑุงู ุฎุงู ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุดู ฺฉุฑุฏ...   \n",
       "4   `` ูุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ '' ุญุงูุฏ...   \n",
       "5                     ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ ๐๐ฅ   \n",
       "6                         ุงูุณุงฺบ ุชฺพฺฉุง ุณูฺูฺบ ุณูุฑ ... ๐๐ฅ   \n",
       "7                               ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู๐๐   \n",
       "8   ุงุฑ ูฺุงุฑ ููุง ููุฏุง ุขุฑ ูฺฏุง ูุง ๐๐ ุชุณ ูพฺฉ...   \n",
       "9                           ุณูุฌฺพุช ูพุงฺฉุณุชุงู ุจูููู ๐๐๐   \n",
       "10                              ุชุณ ูฺุงฺบู ฺฉุฑูุงู ๐๐๐   \n",
       "\n",
       "   Emoji_Punctuation_url_Stopword_removal_cleaned_text  \n",
       "0                      ูู ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ ฺฉูุฌ ูฺบ   \n",
       "1                    ููุงููฺบ ฺฉฺพุงูุง ฺฺู ฺุงฺ ุฏุณุฏ ูฺบ   \n",
       "2   ฺฉุงูุฑุงู ุฎุงู ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุดู ฺฉุฑุฏ...   \n",
       "4        ูุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ ุญุงูุฏ ูุฑ   \n",
       "5                        ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ   \n",
       "6                                ุงูุณุงฺบ ุชฺพฺฉุง ุณูฺูฺบ ุณูุฑ   \n",
       "7                                 ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู   \n",
       "8   ุงุฑ ูฺุงุฑ ููุง ููุฏุง ุขุฑ ูฺฏุง ูุง  ุชุณ ูพฺฉ ู...   \n",
       "9                               ุณูุฌฺพุช ูพุงฺฉุณุชุงู ุจูููู   \n",
       "10                                  ุชุณ ูฺุงฺบู ฺฉุฑูุงู   "
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')  # URL removal pattern\n",
    "hashtag_pattern = re.compile(r'#\\w+')  # Hashtag removal pattern\n",
    "punctuation_marks = string.punctuation + 'ุ'\n",
    "# Regex pattern to remove all punctuation and whitespace\n",
    "punctuation_pattern = re.compile(r\"[{}\\\\s]\".format(re.escape(punctuation_marks)))\n",
    "\n",
    "# Emoji-to-sentiment mapping not implement because Optional\n",
    "emoji_sentiment = {\n",
    "}\n",
    "\n",
    "def replace_emoji_with_sentiment(text):\n",
    "\n",
    "    modified_text = []\n",
    "\n",
    "    for char in text:\n",
    "        if char in emoji_sentiment:\n",
    "            # Replace emoji with corresponding sentiment label\n",
    "            modified_text.append(emoji_sentiment[char])\n",
    "        elif emoji.is_emoji(char):\n",
    "            # If it's an emoji but not in the mapping, skip it\n",
    "            continue\n",
    "        else:\n",
    "            # Add the regular character to the modified text\n",
    "            modified_text.append(char)\n",
    "\n",
    "    # Join the modified characters back into a single string\n",
    "    return ''.join(modified_text)\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        # Remove URLs\n",
    "        text = url_pattern.sub('', text)\n",
    "        \n",
    "        # Remove hashtags\n",
    "        text = hashtag_pattern.sub('', text)\n",
    "        \n",
    "        # Replace emojis with sentiment labels\n",
    "        text = replace_emoji_with_sentiment(text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = punctuation_pattern.sub('', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    return ''\n",
    "\n",
    "def filter_short_posts(text):\n",
    "    if isinstance(text, str):\n",
    "        # Count words in the text\n",
    "        word_count = len(text.split())\n",
    "        return text if word_count >= 3 else ''  # Return the text if it has 3 or more words, else return empty\n",
    "    return ''  \n",
    "\n",
    "df['Emoji_Punctuation_url_removal_cleaned_text'] = df['Stopword_removal_cleaned_text'].apply(clean_text)  # Clean the text\n",
    "df['Emoji_Punctuation_url_Stopword_removal_cleaned_text'] = df['Emoji_Punctuation_url_removal_cleaned_text'].apply(filter_short_posts)  # Filter short posts\n",
    "\n",
    "df = df[df['Emoji_Punctuation_url_Stopword_removal_cleaned_text'] != '']\n",
    "\n",
    "# Display the first 10 lines\n",
    "df[['urdu_text','Stopword_removal_cleaned_text', 'Emoji_Punctuation_url_Stopword_removal_cleaned_text']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emoji_Punctuation_url_removal_cleaned_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ูู ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ ฺฉูุฌ ูฺบ</td>\n",
       "      <td>ู ุดุงุฏ ูุณุงุฏ ูนฺพฺฉ ฺฉูุฌ ู</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ููุงููฺบ ฺฉฺพุงูุง ฺฺู ฺุงฺ ุฏุณุฏ ูฺบ</td>\n",
       "      <td>ูุงู ฺฉฺพุงู ฺฺู ฺุงฺ ุฏุณุฏ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ฺฉุงูุฑุงู ุฎุงู ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุดู ฺฉุฑุฏ...</td>\n",
       "      <td>ฺฉุงูุฑุง ุฎุง ุฏ ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุด ฺฉุฑ ฺฉุด ุงูุฑ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ูุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ ุญุงูุฏ ูุฑ</td>\n",
       "      <td>ุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุฆ ุงุณ ุฆ ุญุงูุฏ ุฑ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ</td>\n",
       "      <td>ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ุงูุณุงฺบ ุชฺพฺฉุง ุณูฺูฺบ ุณูุฑ</td>\n",
       "      <td>ุงูุณุงฺบ ฺพฺฉ ุณูฺ ุณูุฑ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู</td>\n",
       "      <td>ุญุงูุฏ ุฑ ุตุงุญุจ ููฺ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ุงุฑ ูฺุงุฑ ููุง ููุฏุง ุขุฑ ูฺฏุง ูุง  ุชุณ ูพฺฉ ู...</td>\n",
       "      <td>ุงุฑ ูฺุงุฑ ูู ููุฏ ุฑ ูฺฏ ู  ุณ ูพ ูุฌูู ุงุณ ูู...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ุณูุฌฺพุช ูพุงฺฉุณุชุงู ุจูููู</td>\n",
       "      <td>ุณูุฌฺพ ูพุงฺฉุณุชุง ุจูููู</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ุชุณ ูฺุงฺบู ฺฉุฑูุงู</td>\n",
       "      <td>ุณ ูฺุงฺบู ฺฉุฑูุงู</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Emoji_Punctuation_url_removal_cleaned_text  \\\n",
       "0                      ูู ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ ฺฉูุฌ ูฺบ   \n",
       "1                    ููุงููฺบ ฺฉฺพุงูุง ฺฺู ฺุงฺ ุฏุณุฏ ูฺบ   \n",
       "2   ฺฉุงูุฑุงู ุฎุงู ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุดู ฺฉุฑุฏ...   \n",
       "4        ูุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ ุญุงูุฏ ูุฑ   \n",
       "5                        ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ   \n",
       "6                                ุงูุณุงฺบ ุชฺพฺฉุง ุณูฺูฺบ ุณูุฑ   \n",
       "7                                 ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู   \n",
       "8   ุงุฑ ูฺุงุฑ ููุง ููุฏุง ุขุฑ ูฺฏุง ูุง  ุชุณ ูพฺฉ ู...   \n",
       "9                               ุณูุฌฺพุช ูพุงฺฉุณุชุงู ุจูููู   \n",
       "10                                  ุชุณ ูฺุงฺบู ฺฉุฑูุงู   \n",
       "\n",
       "                                         stemmed_text  \n",
       "0                             ู ุดุงุฏ ูุณุงุฏ ูนฺพฺฉ ฺฉูุฌ ู  \n",
       "1                             ูุงู ฺฉฺพุงู ฺฺู ฺุงฺ ุฏุณุฏ   \n",
       "2   ฺฉุงูุฑุง ุฎุง ุฏ ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุด ฺฉุฑ ฺฉุด ุงูุฑ...  \n",
       "4                 ุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุฆ ุงุณ ุฆ ุญุงูุฏ ุฑ  \n",
       "5                        ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ  \n",
       "6                                    ุงูุณุงฺบ ฺพฺฉ ุณูฺ ุณูุฑ  \n",
       "7                                   ุญุงูุฏ ุฑ ุตุงุญุจ ููฺ  \n",
       "8   ุงุฑ ูฺุงุฑ ูู ููุฏ ุฑ ูฺฏ ู  ุณ ูพ ูุฌูู ุงุณ ูู...  \n",
       "9                                  ุณูุฌฺพ ูพุงฺฉุณุชุง ุจูููู  \n",
       "10                                      ุณ ูฺุงฺบู ฺฉุฑูุงู  "
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Urdu prefixes and suffixes\n",
    "urdu_prefixes = [\n",
    "    'ุจ', 'ุบุฑ', 'ูุง', 'ู', 'ูพฺพุฑ', 'ุบูุท', \n",
    "    'ุข', 'ุช', 'ฺฺพููนุง', 'ุจุช', 'ุจุฑ', 'ุณุจ', \n",
    "    'ฺฉฺฺพ', 'ุฎูุฏ'\n",
    "]\n",
    "\n",
    "urdu_suffixes = [\n",
    "    'ฺบ', 'ูฺบ', '', 'ุง', 'ุช', 'ุชุง', \n",
    "    'ุช', 'ฺฏ', 'ฺฏ', 'ฺฉ', 'ฺฉ', 'ฺฉู', \n",
    "    'ฺฉุง', 'ู', 'ูฺบ', 'ฺบ', 'ฺฏุฆฺบ', \n",
    "    'ุฑุง', 'ุฑ', 'ุฑ', 'ูฺบ', 'ุฏุงุฑ', \n",
    "    'ู', 'ุชุงฺบ', 'ุงุฆ', 'ู', ''\n",
    "]\n",
    "\n",
    "def stem_urdu_word(word):\n",
    "    for prefix in urdu_prefixes:\n",
    "        if word.startswith(prefix):\n",
    "            word = word[len(prefix):]  # Remove the prefix\n",
    "\n",
    "    # Remove suffixes\n",
    "    for suffix in urdu_suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]  # Remove the suffix\n",
    "    return word  # Return the original word if no prefix or suffix is found\n",
    "\n",
    "def stem_urdu_text(text):\n",
    "    #Applies the stemming function to each word in the Urdu text.\n",
    "    if isinstance(text, str):\n",
    "        words = text.split()\n",
    "        stemmed_words = [stem_urdu_word(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "    return ''  \n",
    "\n",
    "df['stemmed_text'] = df['Emoji_Punctuation_url_removal_cleaned_text'].apply(stem_urdu_text)\n",
    "\n",
    "# Display the first 10 lines of Stemmed Txt\n",
    "df[['Emoji_Punctuation_url_removal_cleaned_text', 'stemmed_text']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>Emoji_Punctuation_url_removal_cleaned_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>Lemma_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>๐คฃ๐๐ ู ูู ุฏ ูุฑ ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ  ฺฉูุฌ ู...</td>\n",
       "      <td>ูู ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ ฺฉูุฌ ูฺบ</td>\n",
       "      <td>ู ุดุงุฏ ูุณุงุฏ ูนฺพฺฉ ฺฉูุฌ ู</td>\n",
       "      <td>ูู ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ ฺฉูุฌ ูฺบ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ฺู ููุงููฺบ ูฺบ ฺฉฺพุงูุง ุณุฑู ฺฉุฑ ฺฺู ฺุงฺ ููฺบ ุฏุณุฏ...</td>\n",
       "      <td>ููุงููฺบ ฺฉฺพุงูุง ฺฺู ฺุงฺ ุฏุณุฏ ูฺบ</td>\n",
       "      <td>ูุงู ฺฉฺพุงู ฺฺู ฺุงฺ ุฏุณุฏ</td>\n",
       "      <td>ููุงููฺบ ฺฉฺพุงูุง ฺฺู ฺุงฺ ุฏุณุฏ ูฺบ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ฺฉุงูุฑุงู ุฎุงู ุขูพฺฉ ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ฺฏุฆ ุงูพ...</td>\n",
       "      <td>ฺฉุงูุฑุงู ุฎุงู ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุดู ฺฉุฑุฏ...</td>\n",
       "      <td>ฺฉุงูุฑุง ุฎุง ุฏ ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุด ฺฉุฑ ฺฉุด ุงูุฑ...</td>\n",
       "      <td>ฺฉุงูุฑุงู ุฎุงู ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุดู ฺฉุฑุฏ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` ูุฑุงุฏ ุนู ุดุง ฺฉ ุจฺพุณ ูฺบ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ...</td>\n",
       "      <td>ูุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ ุญุงูุฏ ูุฑ</td>\n",
       "      <td>ุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุฆ ุงุณ ุฆ ุญุงูุฏ ุฑ</td>\n",
       "      <td>ูุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ ุญุงูุฏ ูุฑ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ูุงุจู ุงุนุชุจุงุฑ  ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ ูุช ฺบ ๐๐ฅ</td>\n",
       "      <td>ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ</td>\n",
       "      <td>ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ</td>\n",
       "      <td>ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ุงูุณุงฺบ ฺฉู ุชฺพฺฉุง ุฏุชุง  ุณูฺูฺบ ฺฉุง ุณูุฑ ุจฺพ ... ๐๐ฅ</td>\n",
       "      <td>ุงูุณุงฺบ ุชฺพฺฉุง ุณูฺูฺบ ุณูุฑ</td>\n",
       "      <td>ุงูุณุงฺบ ฺพฺฉ ุณูฺ ุณูุฑ</td>\n",
       "      <td>ุงูุณุงฺบ ุชฺพฺฉุง ุณูฺูฺบ ุณูุฑ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู๐๐</td>\n",
       "      <td>ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู</td>\n",
       "      <td>ุญุงูุฏ ุฑ ุตุงุญุจ ููฺ</td>\n",
       "      <td>ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ุงุฑ ูฺุงุฑ ููุง ููุฏุง  ุงุณ ุขุฑ ูฺฏุง ูุง ๐๐ ุช...</td>\n",
       "      <td>ุงุฑ ูฺุงุฑ ููุง ููุฏุง ุขุฑ ูฺฏุง ูุง  ุชุณ ูพฺฉ ู...</td>\n",
       "      <td>ุงุฑ ูฺุงุฑ ูู ููุฏ ุฑ ูฺฏ ู  ุณ ูพ ูุฌูู ุงุณ ูู...</td>\n",
       "      <td>ุงุฑ ูฺุงุฑ ููุง ููุฏุง ุขุฑ ูฺฏุง ูุง  ุชุณ ูพฺฉ ู...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td> ุณูุฌฺพุช ฺบ ุณุงุฑุง ูพุงฺฉุณุชุงู ุจูููู ฺพ ๐๐๐</td>\n",
       "      <td>ุณูุฌฺพุช ูพุงฺฉุณุชุงู ุจูููู</td>\n",
       "      <td>ุณูุฌฺพ ูพุงฺฉุณุชุง ุจูููู</td>\n",
       "      <td>ุณูุฌฺพุช ูพุงฺฉุณุชุงู ุจูููู</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ุชุณ ูฺุงฺบู ฺฉุฑูุงู ุณุงฺ ฺฉ ๐๐๐</td>\n",
       "      <td>ุชุณ ูฺุงฺบู ฺฉุฑูุงู</td>\n",
       "      <td>ุณ ูฺุงฺบู ฺฉุฑูุงู</td>\n",
       "      <td>ุชุณ ูฺุงฺบู ฺฉุฑูุงู</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            urdu_text  \\\n",
       "0   ๐คฃ๐๐ ู ูู ุฏ ูุฑ ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ  ฺฉูุฌ ู...   \n",
       "1   ฺู ููุงููฺบ ูฺบ ฺฉฺพุงูุง ุณุฑู ฺฉุฑ ฺฺู ฺุงฺ ููฺบ ุฏุณุฏ...   \n",
       "2   ฺฉุงูุฑุงู ุฎุงู ุขูพฺฉ ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ฺฏุฆ ุงูพ...   \n",
       "4    `` ูุฑุงุฏ ุนู ุดุง ฺฉ ุจฺพุณ ูฺบ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ...   \n",
       "5         ูุงุจู ุงุนุชุจุงุฑ  ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ ูุช ฺบ ๐๐ฅ   \n",
       "6       ุงูุณุงฺบ ฺฉู ุชฺพฺฉุง ุฏุชุง  ุณูฺูฺบ ฺฉุง ุณูุฑ ุจฺพ ... ๐๐ฅ   \n",
       "7                               ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู๐๐   \n",
       "8   ุงุฑ ูฺุงุฑ ููุง ููุฏุง  ุงุณ ุขุฑ ูฺฏุง ูุง ๐๐ ุช...   \n",
       "9             ุณูุฌฺพุช ฺบ ุณุงุฑุง ูพุงฺฉุณุชุงู ุจูููู ฺพ ๐๐๐   \n",
       "10                      ุชุณ ูฺุงฺบู ฺฉุฑูุงู ุณุงฺ ฺฉ ๐๐๐   \n",
       "\n",
       "           Emoji_Punctuation_url_removal_cleaned_text  \\\n",
       "0                      ูู ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ ฺฉูุฌ ูฺบ   \n",
       "1                    ููุงููฺบ ฺฉฺพุงูุง ฺฺู ฺุงฺ ุฏุณุฏ ูฺบ   \n",
       "2   ฺฉุงูุฑุงู ุฎุงู ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุดู ฺฉุฑุฏ...   \n",
       "4        ูุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ ุญุงูุฏ ูุฑ   \n",
       "5                        ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ   \n",
       "6                                ุงูุณุงฺบ ุชฺพฺฉุง ุณูฺูฺบ ุณูุฑ   \n",
       "7                                 ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู   \n",
       "8   ุงุฑ ูฺุงุฑ ููุง ููุฏุง ุขุฑ ูฺฏุง ูุง  ุชุณ ูพฺฉ ู...   \n",
       "9                               ุณูุฌฺพุช ูพุงฺฉุณุชุงู ุจูููู   \n",
       "10                                  ุชุณ ูฺุงฺบู ฺฉุฑูุงู   \n",
       "\n",
       "                                         stemmed_text  \\\n",
       "0                             ู ุดุงุฏ ูุณุงุฏ ูนฺพฺฉ ฺฉูุฌ ู   \n",
       "1                             ูุงู ฺฉฺพุงู ฺฺู ฺุงฺ ุฏุณุฏ    \n",
       "2   ฺฉุงูุฑุง ุฎุง ุฏ ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุด ฺฉุฑ ฺฉุด ุงูุฑ...   \n",
       "4                 ุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุฆ ุงุณ ุฆ ุญุงูุฏ ุฑ   \n",
       "5                        ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ   \n",
       "6                                    ุงูุณุงฺบ ฺพฺฉ ุณูฺ ุณูุฑ   \n",
       "7                                   ุญุงูุฏ ุฑ ุตุงุญุจ ููฺ   \n",
       "8   ุงุฑ ูฺุงุฑ ูู ููุฏ ุฑ ูฺฏ ู  ุณ ูพ ูุฌูู ุงุณ ูู...   \n",
       "9                                  ุณูุฌฺพ ูพุงฺฉุณุชุง ุจูููู   \n",
       "10                                      ุณ ูฺุงฺบู ฺฉุฑูุงู   \n",
       "\n",
       "                                           Lemma_text  \n",
       "0                      ูู ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ ฺฉูุฌ ูฺบ  \n",
       "1                    ููุงููฺบ ฺฉฺพุงูุง ฺฺู ฺุงฺ ุฏุณุฏ ูฺบ  \n",
       "2   ฺฉุงูุฑุงู ุฎุงู ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุดู ฺฉุฑุฏ...  \n",
       "4        ูุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ ุญุงูุฏ ูุฑ  \n",
       "5                        ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ  \n",
       "6                                ุงูุณุงฺบ ุชฺพฺฉุง ุณูฺูฺบ ุณูุฑ  \n",
       "7                                 ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู  \n",
       "8   ุงุฑ ูฺุงุฑ ููุง ููุฏุง ุขุฑ ูฺฏุง ูุง  ุชุณ ูพฺฉ ู...  \n",
       "9                               ุณูุฌฺพุช ูพุงฺฉุณุชุงู ุจูููู  \n",
       "10                                  ุชุณ ูฺุงฺบู ฺฉุฑูุงู  "
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Urdu lemmatization dictionary\n",
    "urdu_lemmatization_dict = {\n",
    "    'ฺู ุฑ': 'ฺู', 'ฺู ุฑ': 'ฺู', 'ฺูฺบ': 'ฺู', 'ฺูู': 'ฺู',\n",
    "    'ุงฺฺพุง': 'ุงฺฺพุง', 'ุงฺฺพ': 'ุงฺฺพุง', 'ุงฺฺพ': 'ุงฺฺพุง',\n",
    "    'ฺฉุฑฺฏุง': 'ฺฉุฑูุง', 'ฺฉุฑฺฏ': 'ฺฉุฑูุง', 'ฺฉุฑูุง': 'ฺฉุฑูุง', 'ฺฉุฑ': 'ฺฉุฑูุง',\n",
    "    'ูพูฺูุง': 'ูพูฺ', 'ูพูฺ ุฑ': 'ูพูฺ', 'ูพูฺ ุฑ': 'ูพูฺ',\n",
    "    'ฺฉุชุงุจฺบ': 'ฺฉุชุงุจ', 'ฺฉุชุงุจ': 'ฺฉุชุงุจ', 'ฺฉุชุงุจูฺบ': 'ฺฉุชุงุจ',\n",
    "    'ูุญุจุช': 'ูุญุจุช', 'ูุญุจุชฺบ': 'ูุญุจุช',\n",
    "    'ุฎูุด': 'ุฎูุด', 'ุฎูุด': 'ุฎูุด', 'ุฎูุดุงฺบ': 'ุฎูุด',\n",
    "    'ุจุชุฑู': 'ุจุชุฑู', 'ุจุชุฑฺบ': 'ุจุชุฑู',\n",
    "    'ุฏฺฉฺพูุง': 'ุฏฺฉฺพ', 'ุฏฺฉฺพ ุฑ': 'ุฏฺฉฺพ', 'ุฏฺฉฺพ ุฑ': 'ุฏฺฉฺพ',\n",
    "    'ููุง': 'ู', 'ู ุฑุง': 'ู', 'ู ุฑ': 'ู', 'ู ุฑ': 'ู',\n",
    "    'ูฺฉฺพุง': 'ูฺฉฺพ', 'ูฺฉฺพุง ฺฏุง': 'ูฺฉฺพ', 'ูฺฉฺพ ุฑ': 'ูฺฉฺพ', 'ูฺฉฺพุช': 'ูฺฉฺพ',\n",
    "    'ฺุงุชุง': 'ฺุงูุง', 'ฺุงุช': 'ฺุงูุง', 'ฺุงฺบ': 'ฺุงูุง',\n",
    "    'ุฎุฑุฏ': 'ุฎุฑุฏ', 'ุฎุฑุฏฺบ': 'ุฎุฑุฏ',\n",
    "    'ุฏูุง': 'ุฏู', 'ุฏ': 'ุฏู', 'ุฏฺบ': 'ุฏู',\n",
    "    'ฺฏุง': 'ุฌุง', 'ฺฏุฆ': 'ุฌุง', 'ฺฏุฆ': 'ุฌุง',\n",
    "    'ุฎูุดุจู': 'ุฎูุดุจู', 'ุฎูุดุจูุคฺบ': 'ุฎูุดุจู',\n",
    "    'ุณฺฉฺพูุง': 'ุณฺฉฺพ', 'ุณฺฉฺพุงู': 'ุณฺฉฺพ', 'ุณฺฉฺพุงุช': 'ุณฺฉฺพ',\n",
    "    'ูพุงุฑ': 'ูพุงุฑ', 'ูพุงุฑฺบ': 'ูพุงุฑ',\n",
    "    'ฺฉุง': 'ฺฉุฑูุง', 'ฺฉุฑุช': 'ฺฉุฑูุง', 'ฺฉุฑุช': 'ฺฉุฑูุง', 'ฺฉุฑูฺบ': 'ฺฉุฑูุง',\n",
    "    'ฺฺุง': 'ฺฺุง', 'ฺฺุงฺบ': 'ฺฺุง',\n",
    "    'ูพูุง': 'ูพูุง', 'ูพูุงฺบ': 'ูพูุง',\n",
    "    'ฺู': 'ฺู', 'ฺูฺบ': 'ฺู',\n",
    "    'ุฌูฺฏู': 'ุฌูฺฏู', 'ุฌูฺฏูุงุช': 'ุฌูฺฏู',\n",
    "    'ฺุงุช': 'ฺุงุช', 'ฺุงุชฺบ': 'ฺุงุช',\n",
    "    'ุฎูุงุจ': 'ุฎูุงุจ', 'ุฎูุงุจูฺบ': 'ุฎูุงุจ',\n",
    "    'ุฏุฑุฏ': 'ุฏุฑุฏ', 'ุฏุฑุฏูฺบ': 'ุฏุฑุฏ',\n",
    "    'ูพฺุงููุง': 'ูพฺุงู', 'ูพฺุงูฺบ': 'ูพฺุงู',\n",
    "    'ฺฉฺพุงูุง': 'ฺฉฺพุง', 'ฺฉฺพุง ุฑ': 'ฺฉฺพุง', 'ฺฉฺพุง ุฑ': 'ฺฉฺพุง',\n",
    "    'ูุณูุง': 'ูุณ', 'ูุณ ุฑ': 'ูุณ', 'ูุณ ุฑ': 'ูุณ',\n",
    "    'ุณฺฉูู': 'ุณฺฉูู', 'ุณฺฉููุช': 'ุณฺฉูู', 'ุณฺฉููุชฺบ': 'ุณฺฉูู',\n",
    "    'ุณฺ': 'ุณฺ', 'ุณฺ': 'ุณฺ', 'ุณฺ': 'ุณฺ',\n",
    "    'ุฏูฺุณูพ': 'ุฏูฺุณูพ', 'ุฏูฺุณูพุงฺบ': 'ุฏูฺุณูพ',\n",
    "    'ุนุดู': 'ุนุดู', 'ุนุดูฺบ': 'ุนุดู',\n",
    "    'ูุง': 'ูุง', 'ูุฆ': 'ูุง', 'ูุฆ': 'ูุง',\n",
    "    'ูพุฑุงู': 'ูพุฑุงูุง', 'ูพุฑุงูุง': 'ูพุฑุงูุง',\n",
    "    'ูพฺพูู': 'ูพฺพูู', 'ูพฺพูููฺบ': 'ูพฺพูู',\n",
    "    'ุฏุฑุฎุช': 'ุฏุฑุฎุช', 'ุฏุฑุฎุชูฺบ': 'ุฏุฑุฎุช',\n",
    "    'ุงูุณุงู': 'ุงูุณุงู', 'ุงูุณุงููฺบ': 'ุงูุณุงู',\n",
    "    'ุจฺูฺบ': 'ุจฺ', 'ุจฺ': 'ุจฺ',\n",
    "    'ฺฉุชูฺบ': 'ฺฉุชุง', 'ฺฉุชุง': 'ฺฉุชุง',\n",
    "    'ุฏูุณุช': 'ุฏูุณุช', 'ุฏูุณุชูฺบ': 'ุฏูุณุช',\n",
    "    'ุฎุงูุฏุงู': 'ุฎุงูุฏุงู', 'ุฎุงูุฏุงููฺบ': 'ุฎุงูุฏุงู',\n",
    "    'ูุงฺบ': 'ูุงฺบ', 'ูุงุฆฺบ': 'ูุงฺบ',\n",
    "    'ุจุงูพ': 'ุจุงูพ', 'ุจุงูพูฺบ': 'ุจุงูพ',\n",
    "    'ุฑุดุช': 'ุฑุดุช', 'ุฑุดุช': 'ุฑุดุช',\n",
    "    'ุฎูุฏ': 'ุฎูุฏ',\n",
    "    'ุฒูู': 'ุฒูู', 'ุฒููฺบ': 'ุฒูู',\n",
    "    'ูฺฉุงู': 'ูฺฉุงู', 'ูฺฉุงููฺบ': 'ูฺฉุงู',\n",
    "    'ุฏุฑูุงุฒ': 'ุฏุฑูุงุฒ', 'ุฏุฑูุงุฒ': 'ุฏุฑูุงุฒ',\n",
    "    'ฺฉฺพฺฺฉ': 'ฺฉฺพฺฺฉ', 'ฺฉฺพฺฺฉุงฺบ': 'ฺฉฺพฺฺฉ',\n",
    "    'ฺฏฺพุฑ': 'ฺฏฺพุฑ', 'ฺฏฺพุฑูฺบ': 'ฺฏฺพุฑ',\n",
    "    'ฺฉูุฑ': 'ฺฉูุฑ', 'ฺฉูุฑูฺบ': 'ฺฉูุฑ',\n",
    "    'ุฏู': 'ุฏู', 'ุฏููฺบ': 'ุฏู',\n",
    "    'ุฑุงุช': 'ุฑุงุช', 'ุฑุงุชูฺบ': 'ุฑุงุช',\n",
    "    'ููุช': 'ููุช', 'ุงููุงุช': 'ููุช',\n",
    "    'ุฒูุฏฺฏ': 'ุฒูุฏฺฏ', 'ุฒูุฏฺฏุงฺบ': 'ุฒูุฏฺฏ',\n",
    "    'ููุช': 'ููุช', 'ุงููุงุช': 'ููุช',\n",
    "    'ูพุงู': 'ูพุงู', 'ูพุงููฺบ': 'ูพุงู',\n",
    "    'ฺุงุฆ': 'ฺุงุฆ', 'ฺุงุฆ': 'ฺุงุฆ',\n",
    "    'ฺฉฺพุงูุง': 'ฺฉฺพุงูุง', 'ฺฉฺพุงู': 'ฺฉฺพุงูุง'\n",
    "}\n",
    "\n",
    "def lemmatize_urdu_word(word):\n",
    "    #Lemmatizes Urdu words using a predefined dictionary.\n",
    "    return urdu_lemmatization_dict.get(word, word)  # Return the base form if found, else original\n",
    "\n",
    "def lemmatize_urdu_text(text):\n",
    "    #Applies the lemmatization function to each word in the Urdu text.\n",
    "    if isinstance(text, str):\n",
    "        words = text.split()\n",
    "        lemmatized_words = [lemmatize_urdu_word(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "    return ''\n",
    "# Apply lemmatization\n",
    "df['Lemma_text'] = df['Emoji_Punctuation_url_removal_cleaned_text'].apply(lemmatize_urdu_text)\n",
    "\n",
    "df[['urdu_text','Emoji_Punctuation_url_removal_cleaned_text','stemmed_text','Lemma_text']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemma_text</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ูู ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ ฺฉูุฌ ูฺบ</td>\n",
       "      <td>[ูู, ุดุงุฏ, ูุณุงุฏู, ูนฺพฺฉ, ฺฉูุฌ, ูฺบ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ููุงููฺบ ฺฉฺพุงูุง ฺฺู ฺุงฺ ุฏุณุฏ ูฺบ</td>\n",
       "      <td>[ููุงููฺบ, ฺฉฺพุงูุง, ฺฺู, ฺุงฺ, ุฏุณุฏ, ูฺบ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ฺฉุงูุฑุงู ุฎุงู ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุดู ฺฉุฑุฏ...</td>\n",
       "      <td>[ฺฉุงูุฑุงู, ุฎุงู, ุฏู, ุจฺพุฑ, ุฒู, ุฏุงุฑ, ูฺฏุงุฆ, ุงูพู...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ูุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ ุญุงูุฏ ูุฑ</td>\n",
       "      <td>[ูุฑุงุฏ, ุนู, ุดุง, ุจฺพุณ, ฺ, ุฌ, ุขุฆ, ุงุณ, ุขุฆ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ</td>\n",
       "      <td>[ูุงุจู, ุงุนุชุจุงุฑ, ุงฺฉุซุฑ, ูุงุชู, ุงุนุชุจุงุฑ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ุงูุณุงฺบ ุชฺพฺฉุง ุณูฺูฺบ ุณูุฑ</td>\n",
       "      <td>[ุงูุณุงฺบ, ุชฺพฺฉุง, ุณูฺูฺบ, ุณูุฑ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู</td>\n",
       "      <td>[ุญุงูุฏ, ูุฑ, ุตุงุญุจ, ููฺู]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ุงุฑ ูฺุงุฑ ููุง ููุฏุง ุขุฑ ูฺฏุง ูุง  ุชุณ ูพฺฉ ู...</td>\n",
       "      <td>[ุงุฑ, ูฺุงุฑ, ููุง, ููุฏุง, ุขุฑ, ูฺฏุง, ูุง, , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ุณูุฌฺพุช ูพุงฺฉุณุชุงู ุจูููู</td>\n",
       "      <td>[ุณูุฌฺพุช, ูพุงฺฉุณุชุงู, ุจูููู]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ุชุณ ูฺุงฺบู ฺฉุฑูุงู</td>\n",
       "      <td>[ุชุณ, ูฺุงฺบู, ฺฉุฑูุงู]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Lemma_text  \\\n",
       "0                      ูู ุดุงุฏ ูุณุงุฏู ูนฺพฺฉ ฺฉูุฌ ูฺบ   \n",
       "1                    ููุงููฺบ ฺฉฺพุงูุง ฺฺู ฺุงฺ ุฏุณุฏ ูฺบ   \n",
       "2   ฺฉุงูุฑุงู ุฎุงู ุฏู ุจฺพุฑ ุฒู ุฏุงุฑ ูฺฏุงุฆ ุงูพูุฒุดู ฺฉุฑุฏ...   \n",
       "4        ูุฑุงุฏ ุนู ุดุง ุจฺพุณ ฺ ุฌ ุขุฆ ุงุณ ุขุฆ ุญุงูุฏ ูุฑ   \n",
       "5                        ูุงุจู ุงุนุชุจุงุฑ ุงฺฉุซุฑ ูุงุชู ุงุนุชุจุงุฑ   \n",
       "6                                ุงูุณุงฺบ ุชฺพฺฉุง ุณูฺูฺบ ุณูุฑ   \n",
       "7                                 ุญุงูุฏ ูุฑ ุตุงุญุจ ููฺู   \n",
       "8   ุงุฑ ูฺุงุฑ ููุง ููุฏุง ุขุฑ ูฺฏุง ูุง  ุชุณ ูพฺฉ ู...   \n",
       "9                               ุณูุฌฺพุช ูพุงฺฉุณุชุงู ุจูููู   \n",
       "10                                  ุชุณ ูฺุงฺบู ฺฉุฑูุงู   \n",
       "\n",
       "                                           token_text  \n",
       "0               [ูู, ุดุงุฏ, ูุณุงุฏู, ูนฺพฺฉ, ฺฉูุฌ, ูฺบ]  \n",
       "1             [ููุงููฺบ, ฺฉฺพุงูุง, ฺฺู, ฺุงฺ, ุฏุณุฏ, ูฺบ]  \n",
       "2   [ฺฉุงูุฑุงู, ุฎุงู, ุฏู, ุจฺพุฑ, ุฒู, ุฏุงุฑ, ูฺฏุงุฆ, ุงูพู...  \n",
       "4   [ูุฑุงุฏ, ุนู, ุดุง, ุจฺพุณ, ฺ, ุฌ, ุขุฆ, ุงุณ, ุขุฆ, ...  \n",
       "5                  [ูุงุจู, ุงุนุชุจุงุฑ, ุงฺฉุซุฑ, ูุงุชู, ุงุนุชุจุงุฑ]  \n",
       "6                           [ุงูุณุงฺบ, ุชฺพฺฉุง, ุณูฺูฺบ, ุณูุฑ]  \n",
       "7                            [ุญุงูุฏ, ูุฑ, ุตุงุญุจ, ููฺู]  \n",
       "8   [ุงุฑ, ูฺุงุฑ, ููุง, ููุฏุง, ุขุฑ, ูฺฏุง, ูุง, , ...  \n",
       "9                           [ุณูุฌฺพุช, ูพุงฺฉุณุชุงู, ุจูููู]  \n",
       "10                              [ุชุณ, ูฺุงฺบู, ฺฉุฑูุงู]  "
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load the multilingual model\n",
    "nlp = spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "# Tokenization function using SpaCy\n",
    "def tokenize_urdu(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "# Appply Tokenized\n",
    "df['token_text'] = df['Lemma_text'].apply(tokenize_urdu)\n",
    "df[['Lemma_text','token_text']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6302</th>\n",
       "      <td>ุชู</td>\n",
       "      <td>499.924465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>ุฏู</td>\n",
       "      <td>243.606889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15592</th>\n",
       "      <td>ู</td>\n",
       "      <td>198.743859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2410</th>\n",
       "      <td>ุงุจ</td>\n",
       "      <td>194.253786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19734</th>\n",
       "      <td>ฺฉูุฆ</td>\n",
       "      <td>182.443432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3194</th>\n",
       "      <td>ุงูู</td>\n",
       "      <td>179.840199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7703</th>\n",
       "      <td>ุฎุงู</td>\n",
       "      <td>175.210094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7271</th>\n",
       "      <td>ุฌ</td>\n",
       "      <td>166.572750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4240</th>\n",
       "      <td>ุจุงุช</td>\n",
       "      <td>165.799989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19457</th>\n",
       "      <td>ฺฉุฑฺบ</td>\n",
       "      <td>158.604370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  TF-IDF Score\n",
       "6302     ุชู    499.924465\n",
       "8291     ุฏู    243.606889\n",
       "15592    ู    198.743859\n",
       "2410     ุงุจ    194.253786\n",
       "19734  ฺฉูุฆ    182.443432\n",
       "3194   ุงูู    179.840199\n",
       "7703    ุฎุงู    175.210094\n",
       "7271     ุฌ    166.572750\n",
       "4240    ุจุงุช    165.799989\n",
       "19457  ฺฉุฑฺบ    158.604370"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load Urdu language model for spaCy\n",
    "nlp = spacy.blank(\"ur\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_urdu(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc if token.text not in urdu_stopwords]\n",
    "# Tokenizing the 'urdu_text' column\n",
    "df['TF_IDF_token_text'] = df['Lemma_text'].apply(tokenize_urdu)\n",
    "\n",
    "# Joining the tokenized lists back into strings for TF-IDF processing\n",
    "df['TF_IDF_token_text'] = df['TF_IDF_token_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# TF-IDF Vectorization with stop words removed\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['TF_IDF_token_text'])\n",
    "\n",
    "# Getting the TF-IDF scores\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame\n",
    "tfidf_df = pd.DataFrame({'Word': words, 'TF-IDF Score': tfidf_scores})\n",
    "tfidf_df = tfidf_df.sort_values(by='TF-IDF Score', ascending=False)\n",
    "\n",
    "# Display the top 10 words with the highest TF-IDF scores\n",
    "tfidf_df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Words Most Similar to 'ุงฺฺพุง' (Good):\n",
      "ูุช: 0.989397406578064\n",
      "ุชฺฉูู: 0.9884074330329895\n",
      "ุฏู: 0.9862785339355469\n",
      "ูุญุจุช: 0.9856767058372498\n",
      "ููุช: 0.9847457408905029\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# Train Word2Vec model on tokenized Urdu data\n",
    "#Take Word2Vec list of tokenized sentences\n",
    "sentences = df['token_text'].tolist()\n",
    "\n",
    "# Train the model\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Find top 5 words most similar to 'ุงฺฺพุง' (\"good\")\n",
    "similar_words = word2vec_model.wv.most_similar(\"ุงฺฺพุง\", topn=5)\n",
    "\n",
    "# Display most similar words\n",
    "print(\"Top 5 Words Most Similar to 'ุงฺฺพุง' (Good):\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Common Bigrams with Frequencies:\n",
      "ุนูุฑุงู ุฎุงู: 500\n",
      "ุ ุ: 479\n",
      "ููุงุฒ ุดุฑู: 446\n",
      "ุขุฆ ุฌ: 312\n",
      "ุณูุฏฺพ ูพููุณ: 299\n",
      "ุขุฑู ฺู: 223\n",
      "ุฎุงู ุตุงุญุจ: 181\n",
      "ฺฉูพูนู ุตูุฏุฑ: 177\n",
      "๐ต ๐ฐ: 162\n",
      "ุฌุฒุงฺฉ ุงูู: 158\n",
      "\n",
      "Top 10 Most Common Trigrams with Frequencies:\n",
      "ุ ุ ุ: 226\n",
      "ุขุฆ ุฌ ุณูุฏฺพ: 118\n",
      "ูพ ูน ุขุฆ: 114\n",
      "ูพ ฺ ุงู: 86\n",
      "ุตู ุงูู ุนู: 86\n",
      "ูุงูู ฺฉุฑฺบ ูุงูู: 74\n",
      "ุฌุฒุงฺฉ ุงูู ุฎุฑ: 71\n",
      "ฺฉุฑฺบ ูุงูู ุจฺฉ: 71\n",
      "ุขุฆ ุฌ ุงุบูุง: 71\n",
      "ูุงููฺบ ูุงูู ฺฉุฑฺบ: 69\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "# Load Urdu tokenizer using spaCy \n",
    "nlp = spacy.blank(\"ur\")\n",
    "\n",
    "#Function to generate n-grams\n",
    "def get_ngrams(tokenized_text, n):\n",
    "    return list(ngrams(tokenized_text, n))\n",
    "\n",
    "# Generate unigrams, bigrams, and trigrams from the tokenized text\n",
    "df['unigrams'] = df['token_text'].apply(lambda x: get_ngrams(x, 1))\n",
    "df['bigrams'] = df['token_text'].apply(lambda x: get_ngrams(x, 2))\n",
    "df['trigrams'] = df['token_text'].apply(lambda x: get_ngrams(x, 3))\n",
    "\n",
    "#Flatten the list of bigrams and trigrams for frequency counting\n",
    "all_bigrams = [bigram for sublist in df['bigrams'] for bigram in sublist]\n",
    "all_trigrams = [trigram for sublist in df['trigrams'] for trigram in sublist]\n",
    "\n",
    "#Count the frequency of bigrams and trigrams\n",
    "bigram_freq = Counter(all_bigrams)\n",
    "trigram_freq = Counter(all_trigrams)\n",
    "\n",
    "#Get the top 10 most common bigrams and trigrams\n",
    "top_10_bigrams = bigram_freq.most_common(10)\n",
    "top_10_trigrams = trigram_freq.most_common(10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 Most Common Bigrams with Frequencies:\")\n",
    "for bigram, freq in top_10_bigrams:\n",
    "    print(f\"{' '.join(bigram)}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 10 Most Common Trigrams with Frequencies:\")\n",
    "for trigram, freq in top_10_trigrams:\n",
    "    print(f\"{' '.join(trigram)}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Metrics:\n",
      "Accuracy: 0.7797\n",
      "Precision: 0.7802\n",
      "Recall: 0.7797\n",
      "F1-Score: 0.7794\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#Prepare the data\n",
    "X = df['TF_IDF_token_text']\n",
    "y = df['is_sarcastic']\n",
    "\n",
    "#Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Create a TF-IDF vectorizer and logistic regression model pipeline\n",
    "model = make_pipeline(TfidfVectorizer(), LogisticRegression(max_iter=1000))\n",
    "\n",
    "#Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "#Display the results\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.75      0.76      1775\n",
      "         1.0       0.78      0.81      0.80      1979\n",
      "\n",
      "    accuracy                           0.78      3754\n",
      "   macro avg       0.78      0.78      0.78      3754\n",
      "weighted avg       0.78      0.78      0.78      3754\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression for Sentiment Classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, df['is_sarcastic'], test_size=0.2)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.75      0.76      1775\n",
      "         1.0       0.78      0.81      0.80      1979\n",
      "\n",
      "    accuracy                           0.78      3754\n",
      "   macro avg       0.78      0.78      0.78      3754\n",
      "weighted avg       0.78      0.78      0.78      3754\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation Metrics\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the model performance using test set predictions\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified texts:\n",
      "  (0, 8763)\t0.43961025138992377\n",
      "  (0, 20810)\t0.47452896039611114\n",
      "  (0, 5325)\t0.2693137041028108\n",
      "  (0, 13138)\t0.40842531485148803\n",
      "  (0, 16956)\t0.320427025847638\n",
      "  (0, 19137)\t0.3538912232974851\n",
      "  (0, 16233)\t0.3076619447443641\n",
      "  (0, 6302)\t0.14019855393235375\n",
      "  (1, 16300)\t0.2697738438201112\n",
      "  (1, 4506)\t0.2610282206655509\n",
      "  (1, 5754)\t0.27832713125679903\n",
      "  (1, 5315)\t0.29498046016561186\n",
      "  (1, 11048)\t0.5052455659180036\n",
      "  (1, 17707)\t0.2899550231999509\n",
      "  (1, 11947)\t0.20706008976687007\n",
      "  (1, 7437)\t0.21013066923355878\n",
      "  (1, 4256)\t0.21781031387708094\n",
      "  (1, 18788)\t0.2526227829590018\n",
      "  (1, 20753)\t0.18733288896228817\n",
      "  (1, 12972)\t0.17418113474920308\n",
      "  (1, 16354)\t0.16810396755622672\n",
      "  (1, 8652)\t0.160608283837478\n",
      "  (1, 8291)\t0.11252779955310588\n",
      "  (1, 6302)\t0.08324596379893331\n",
      "  (1, 11239)\t0.13376269229850715\n",
      "  :\t:\n",
      "  (823, 6826)\t0.16938233303491282\n",
      "  (824, 8896)\t0.36314900338650125\n",
      "  (824, 12504)\t0.3135345392808945\n",
      "  (824, 4383)\t0.2742160067305132\n",
      "  (824, 18429)\t0.2559339302211\n",
      "  (824, 14771)\t0.2700006377881608\n",
      "  (824, 161)\t0.26744317358506625\n",
      "  (824, 17815)\t0.2445367034451695\n",
      "  (824, 9452)\t0.25970470623293535\n",
      "  (824, 12030)\t0.24654410454836145\n",
      "  (824, 6682)\t0.18654778022521615\n",
      "  (824, 9717)\t0.1976245995732084\n",
      "  (824, 13110)\t0.2559339302211\n",
      "  (824, 5759)\t0.2302429675729785\n",
      "  (824, 13285)\t0.2224318316260118\n",
      "  (824, 21681)\t0.1716948134510436\n",
      "  (824, 13341)\t0.15919418382694547\n",
      "  (825, 13246)\t0.2584380756174065\n",
      "  (825, 7879)\t0.24012558196971903\n",
      "  (825, 6866)\t0.45625694365178526\n",
      "  (825, 13634)\t0.3427340799571628\n",
      "  (825, 15466)\t0.6051589248596619\n",
      "  (825, 18540)\t0.2685088673796414\n",
      "  (825, 2047)\t0.2726306814649602\n",
      "  (825, 2410)\t0.19305593253115974\n"
     ]
    }
   ],
   "source": [
    "# Error Analysis\n",
    "\n",
    "#finding misclassified examples\n",
    "misclassified = X_test[(y_test != y_pred)]\n",
    "print(\"Misclassified texts:\")\n",
    "print(misclassified)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
