{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نہیں پائین 😎</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>حامد میر صاحب ویلڈن👏😊</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 ت...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  is_sarcastic  \\\n",
       "0  🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...           1.0   \n",
       "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...           1.0   \n",
       "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...           0.0   \n",
       "3                                       نہیں پائین 😎           0.0   \n",
       "4   `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...           1.0   \n",
       "5        قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥           1.0   \n",
       "6      انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀           0.0   \n",
       "7                              حامد میر صاحب ویلڈن👏😊           0.0   \n",
       "8  یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 ت...           1.0   \n",
       "9           یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂           1.0   \n",
       "\n",
       "   Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5 Unnamed: 6  Unnamed: 7  \n",
       "0         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "1         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "2         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "3         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "4         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "5         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "6         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "7         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "8         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "9         NaN         NaN         NaN         NaN        NaN         NaN  "
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load data\n",
    "file_path = 'urdu_sarcastic_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first 10 lines of Datset\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>Stopword_removal_cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...</td>\n",
       "      <td>🤣😂😂 لینے شادی فسادن ٹھیک کوجی نہیں 😐😐😐🤣</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "      <td>مہمانوں کھانا چڑیل چاچی دسدی میں😂😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "      <td>کامران خان دن بھریہ زمہ داری لگائی اپوزیشن کرد...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نہیں پائین 😎</td>\n",
       "      <td>نہیں پائین 😎</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...</td>\n",
       "      <td>`` مراد علی شاہ بھیس ڈی جی آئی ایس آئی '' حامد...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥</td>\n",
       "      <td>قابل اعتبار اکثر قاتل اعتبار 💔🔥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀</td>\n",
       "      <td>انساں تھکا سوچوں سفر ... 🍁🥀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>حامد میر صاحب ویلڈن👏😊</td>\n",
       "      <td>حامد میر صاحب ویلڈن👏😊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 ت...</td>\n",
       "      <td>یار وچارہ ویلا ہوندا آرے لگا ہویا ہے😂😂 تسی پکے...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂</td>\n",
       "      <td>سمجھتے پاکستان بیوقوف 😂😂😂</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  \\\n",
       "0  🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...   \n",
       "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...   \n",
       "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...   \n",
       "3                                       نہیں پائین 😎   \n",
       "4   `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...   \n",
       "5        قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥   \n",
       "6      انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀   \n",
       "7                              حامد میر صاحب ویلڈن👏😊   \n",
       "8  یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 ت...   \n",
       "9           یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂   \n",
       "\n",
       "                       Stopword_removal_cleaned_text  \n",
       "0            🤣😂😂 لینے شادی فسادن ٹھیک کوجی نہیں 😐😐😐🤣  \n",
       "1                 مہمانوں کھانا چڑیل چاچی دسدی میں😂😂  \n",
       "2  کامران خان دن بھریہ زمہ داری لگائی اپوزیشن کرد...  \n",
       "3                                       نہیں پائین 😎  \n",
       "4  `` مراد علی شاہ بھیس ڈی جی آئی ایس آئی '' حامد...  \n",
       "5                    قابل اعتبار اکثر قاتل اعتبار 💔🔥  \n",
       "6                        انساں تھکا سوچوں سفر ... 🍁🥀  \n",
       "7                              حامد میر صاحب ویلڈن👏😊  \n",
       "8  یار وچارہ ویلا ہوندا آرے لگا ہویا ہے😂😂 تسی پکے...  \n",
       "9                          سمجھتے پاکستان بیوقوف 😂😂😂  "
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "exception_stopwords = [\n",
    "    'نہیں',  # No, Not\n",
    "    'برا',   # Bad\n",
    "    'اچھا',  # Good\n",
    "    'بہتر',  # Better\n",
    "    'خراب',  # Bad/Destroyed\n",
    "    'غلط',   # Wrong\n",
    "    'سچ',    # True\n",
    "    'جھوٹ',  # False/Lie\n",
    "    'خوش',   # Happy\n",
    "    'اداس',  # Sad\n",
    "    'محبت',  # Love\n",
    "    'نفرت',  # Hate\n",
    "    'خوف',   # Fear\n",
    "    'خوبصورت', # Beautiful\n",
    "    'بدصورت', # Ugly\n",
    "    'زیادہ',  # More\n",
    "    'کم',     # Less\n",
    "    'بہت',    # Very\n",
    "    'انتہائی', # Extremely\n",
    "    'براہ',   # Wrongly\n",
    "    'معمولی', # Slight\n",
    "    'حقیقت',  # Reality\n",
    "    'بےکار',  # Useless\n",
    "    'پیار',   # Love\n",
    "    'ظالم',   # Cruel\n",
    "    'نیک',    # Good/Pious\n",
    "    'غریب',   # Poor\n",
    "    'امیر',   # Rich\n",
    "    'سست',    # Slow/Lazy\n",
    "    'تیز',    # Fast\n",
    "]\n",
    "\n",
    "# Function to remove stopwords, excluding exception words\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        words = text.split()  # Tokenize the text\n",
    "        # Only include words that are not in urdu_stopwords, or are in exception_stopwords\n",
    "        cleaned_words = [word for word in words if word not in urdu_stopwords or word in exception_stopwords]\n",
    "        return ' '.join(cleaned_words)  # Join the words back into a string\n",
    "    return '' \n",
    "urdu_stopwords = {\n",
    "    \"وہ\", \"میں\", \"میری\", \"میرا\", \"میرے\", \"اسے\", \"اسکا\", \"اسکے\", \"اسکی\", \"گا\",\n",
    "    \"گی\", \"گے\", \"ہم\", \"ہمیں\", \"ہمارے\", \"ہمارا\", \"اور\", \"کیا\", \"کیوں\", \"لیے\",\n",
    "    \"پر\", \"ہے\", \"کہ\", \"سے\", \"کا\", \"کے\", \"تھا\", \"تھے\", \"تھی\", \"ہو\",\n",
    "    \"ایک\", \"تم\", \"تمہارے\", \"تمہارا\", \"تمہاری\", \"تمہیں\", \"آپ\", \"آپکا\", \"آپکی\", \"آپکے\",\n",
    "    \"ہیں\", \"اس\", \"ان\", \"بھی\", \"یا\", \"نہیں\", \"کچھ\", \"ہر\", \"مگر\", \"جو\",\n",
    "    \"انکا\", \"انکی\", \"انکے\", \"اپنا\", \"اپنی\", \"اپنے\", \"ابھی\", \"بہت\", \"صرف\", \"لیکن\",\n",
    "    \"پھر\", \"یہ\", \"وہاں\", \"کہاں\", \"کس\", \"کب\", \"کیسے\", \"چاہیے\", \"آج\", \"کل\",\n",
    "    \"پر\", \"تک\", \"یہاں\", \"وہاں\", \"اگر\", \"کی\", \"موسم\", \"کیوں\", \"بھی\", \"ہو سکتا\",\n",
    "    \"بہت زیادہ\", \"اس لیے\", \"آتا\", \"گیا\", \"دیا\", \"چلا\", \"بنا\", \"کیا\", \"دیکھا\", \"ہوئے\",\n",
    "    \"آتا\", \"چاہتا\", \"ہوتا\", \"رہتا\", \"چاہتا\", \"پاتا\", \"پہنچتا\", \"دیکھتا\", \"ہوتا\", \"کرتا\",\n",
    "    \"چلو\", \"ہوئی\", \"گئے\", \"گئی\", \"چلا گیا\", \"آئے\", \"کوشش\", \"کہ\", \"کرنے\", \"کرنا\",\n",
    "    \"یاد\", \"چاہت\", \"تھا\", \"تھا\", \"رہی\", \"رہا\", \"کر رہے\", \"کر رہا\", \"آئے\", \"آتا\",\n",
    "    'بھر','یہ', 'میں', 'اور', 'کا', 'ہے', 'کی', 'کے', 'ہے', 'تھا', 'تھی', 'ہوں', 'آپ', 'نے', \n",
    "    'آں','نوں','چل','کیا', 'پر', 'کچھ', 'ہم', 'ہوں', 'کہ', 'نہیں', 'بھی', 'جو', 'تو'  # Add 'تو' here\n",
    "\"کرئیے\",\"رکھے\",\"رہیگا\",\"تے\",\"سی\",\"نے\",\"اپ\",\"ساڈی\",\"انی\",\"کرو\",\"سارا\",\"ھے\",\"ہے\",\"دیتا\",\"کو\",\"ہوتے\",\"ہی\",\"آپ\",\"کرتی\",\"سے\",\"سب\",\"دے\",\"سرو\",\"کر\",\"آئے\", \"آتا\", \"پہنچتا\", \"آتا\", \"چاہتا\", \"آتا\",\n",
    "}\n",
    "\n",
    "# By Peform Stopword Reoving to the 'urdu_text' column\n",
    "df['Stopword_removal_cleaned_text'] = df['urdu_text'].apply(remove_stopwords)\n",
    "\n",
    "# Display the first 10 lines after removing Stopwords\n",
    "df[['urdu_text','Stopword_removal_cleaned_text']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>Stopword_removal_cleaned_text</th>\n",
       "      <th>Emoji_Punctuation_url_Stopword_removal_cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...</td>\n",
       "      <td>🤣😂😂 لینے شادی فسادن ٹھیک کوجی نہیں 😐😐😐🤣</td>\n",
       "      <td>لینے شادی فسادن ٹھیک کوجی نہیں</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "      <td>مہمانوں کھانا چڑیل چاچی دسدی میں😂😂</td>\n",
       "      <td>مہمانوں کھانا چڑیل چاچی دسدی میں</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "      <td>کامران خان دن بھریہ زمہ داری لگائی اپوزیشن کرد...</td>\n",
       "      <td>کامران خان دن بھریہ زمہ داری لگائی اپوزیشن کرد...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...</td>\n",
       "      <td>`` مراد علی شاہ بھیس ڈی جی آئی ایس آئی '' حامد...</td>\n",
       "      <td>مراد علی شاہ بھیس ڈی جی آئی ایس آئی حامد میر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥</td>\n",
       "      <td>قابل اعتبار اکثر قاتل اعتبار 💔🔥</td>\n",
       "      <td>قابل اعتبار اکثر قاتل اعتبار</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀</td>\n",
       "      <td>انساں تھکا سوچوں سفر ... 🍁🥀</td>\n",
       "      <td>انساں تھکا سوچوں سفر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>حامد میر صاحب ویلڈن👏😊</td>\n",
       "      <td>حامد میر صاحب ویلڈن👏😊</td>\n",
       "      <td>حامد میر صاحب ویلڈن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 ت...</td>\n",
       "      <td>یار وچارہ ویلا ہوندا آرے لگا ہویا ہے😂😂 تسی پکے...</td>\n",
       "      <td>یار وچارہ ویلا ہوندا آرے لگا ہویا ہے تسی پکے ن...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂</td>\n",
       "      <td>سمجھتے پاکستان بیوقوف 😂😂😂</td>\n",
       "      <td>سمجھتے پاکستان بیوقوف</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>تسی لڑاںٔی کروانی ساڈی کی 😂😂😂</td>\n",
       "      <td>تسی لڑاںٔی کروانی 😂😂😂</td>\n",
       "      <td>تسی لڑاںٔی کروانی</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            urdu_text  \\\n",
       "0   🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...   \n",
       "1   چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...   \n",
       "2   کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...   \n",
       "4    `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...   \n",
       "5         قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥   \n",
       "6       انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀   \n",
       "7                               حامد میر صاحب ویلڈن👏😊   \n",
       "8   یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 ت...   \n",
       "9            یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂   \n",
       "10                      تسی لڑاںٔی کروانی ساڈی کی 😂😂😂   \n",
       "\n",
       "                        Stopword_removal_cleaned_text  \\\n",
       "0             🤣😂😂 لینے شادی فسادن ٹھیک کوجی نہیں 😐😐😐🤣   \n",
       "1                  مہمانوں کھانا چڑیل چاچی دسدی میں😂😂   \n",
       "2   کامران خان دن بھریہ زمہ داری لگائی اپوزیشن کرد...   \n",
       "4   `` مراد علی شاہ بھیس ڈی جی آئی ایس آئی '' حامد...   \n",
       "5                     قابل اعتبار اکثر قاتل اعتبار 💔🔥   \n",
       "6                         انساں تھکا سوچوں سفر ... 🍁🥀   \n",
       "7                               حامد میر صاحب ویلڈن👏😊   \n",
       "8   یار وچارہ ویلا ہوندا آرے لگا ہویا ہے😂😂 تسی پکے...   \n",
       "9                           سمجھتے پاکستان بیوقوف 😂😂😂   \n",
       "10                              تسی لڑاںٔی کروانی 😂😂😂   \n",
       "\n",
       "   Emoji_Punctuation_url_Stopword_removal_cleaned_text  \n",
       "0                      لینے شادی فسادن ٹھیک کوجی نہیں   \n",
       "1                    مہمانوں کھانا چڑیل چاچی دسدی میں   \n",
       "2   کامران خان دن بھریہ زمہ داری لگائی اپوزیشن کرد...   \n",
       "4        مراد علی شاہ بھیس ڈی جی آئی ایس آئی حامد میر   \n",
       "5                        قابل اعتبار اکثر قاتل اعتبار   \n",
       "6                                انساں تھکا سوچوں سفر   \n",
       "7                                 حامد میر صاحب ویلڈن   \n",
       "8   یار وچارہ ویلا ہوندا آرے لگا ہویا ہے تسی پکے ن...   \n",
       "9                               سمجھتے پاکستان بیوقوف   \n",
       "10                                  تسی لڑاںٔی کروانی   "
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')  # URL removal pattern\n",
    "hashtag_pattern = re.compile(r'#\\w+')  # Hashtag removal pattern\n",
    "punctuation_marks = string.punctuation + '،۔'\n",
    "# Regex pattern to remove all punctuation and whitespace\n",
    "punctuation_pattern = re.compile(r\"[{}\\\\s]\".format(re.escape(punctuation_marks)))\n",
    "\n",
    "# Emoji-to-sentiment mapping not implement because Optional\n",
    "emoji_sentiment = {\n",
    "}\n",
    "\n",
    "def replace_emoji_with_sentiment(text):\n",
    "\n",
    "    modified_text = []\n",
    "\n",
    "    for char in text:\n",
    "        if char in emoji_sentiment:\n",
    "            # Replace emoji with corresponding sentiment label\n",
    "            modified_text.append(emoji_sentiment[char])\n",
    "        elif emoji.is_emoji(char):\n",
    "            # If it's an emoji but not in the mapping, skip it\n",
    "            continue\n",
    "        else:\n",
    "            # Add the regular character to the modified text\n",
    "            modified_text.append(char)\n",
    "\n",
    "    # Join the modified characters back into a single string\n",
    "    return ''.join(modified_text)\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        # Remove URLs\n",
    "        text = url_pattern.sub('', text)\n",
    "        \n",
    "        # Remove hashtags\n",
    "        text = hashtag_pattern.sub('', text)\n",
    "        \n",
    "        # Replace emojis with sentiment labels\n",
    "        text = replace_emoji_with_sentiment(text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = punctuation_pattern.sub('', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    return ''\n",
    "\n",
    "def filter_short_posts(text):\n",
    "    if isinstance(text, str):\n",
    "        # Count words in the text\n",
    "        word_count = len(text.split())\n",
    "        return text if word_count >= 3 else ''  # Return the text if it has 3 or more words, else return empty\n",
    "    return ''  \n",
    "\n",
    "df['Emoji_Punctuation_url_removal_cleaned_text'] = df['Stopword_removal_cleaned_text'].apply(clean_text)  # Clean the text\n",
    "df['Emoji_Punctuation_url_Stopword_removal_cleaned_text'] = df['Emoji_Punctuation_url_removal_cleaned_text'].apply(filter_short_posts)  # Filter short posts\n",
    "\n",
    "df = df[df['Emoji_Punctuation_url_Stopword_removal_cleaned_text'] != '']\n",
    "\n",
    "# Display the first 10 lines\n",
    "df[['urdu_text','Stopword_removal_cleaned_text', 'Emoji_Punctuation_url_Stopword_removal_cleaned_text']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emoji_Punctuation_url_removal_cleaned_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>لینے شادی فسادن ٹھیک کوجی نہیں</td>\n",
       "      <td>لی شاد فساد ٹھیک کوج نہ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>مہمانوں کھانا چڑیل چاچی دسدی میں</td>\n",
       "      <td>ہمان کھان چڑیل چاچ دسد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان دن بھریہ زمہ داری لگائی اپوزیشن کرد...</td>\n",
       "      <td>کامرا خا د بھریہ زمہ دار لگائ اپوزیش کر کش اور...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مراد علی شاہ بھیس ڈی جی آئی ایس آئی حامد میر</td>\n",
       "      <td>راد عل شاہ بھیس ڈ ج ئ ایس ئ حامد یر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>قابل اعتبار اکثر قاتل اعتبار</td>\n",
       "      <td>قابل اعتبار اکثر قاتل اعتبار</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>انساں تھکا سوچوں سفر</td>\n",
       "      <td>انساں ھک سوچ سفر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>حامد میر صاحب ویلڈن</td>\n",
       "      <td>حامد یر صاحب ویلڈ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>یار وچارہ ویلا ہوندا آرے لگا ہویا ہے تسی پکے ن...</td>\n",
       "      <td>یار وچارہ ویل ہوند رے لگ ہوی ہے س پ نجوم اس نن...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>سمجھتے پاکستان بیوقوف</td>\n",
       "      <td>سمجھ پاکستا بیوقوف</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>تسی لڑاںٔی کروانی</td>\n",
       "      <td>س لڑاںٔ کروان</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Emoji_Punctuation_url_removal_cleaned_text  \\\n",
       "0                      لینے شادی فسادن ٹھیک کوجی نہیں   \n",
       "1                    مہمانوں کھانا چڑیل چاچی دسدی میں   \n",
       "2   کامران خان دن بھریہ زمہ داری لگائی اپوزیشن کرد...   \n",
       "4        مراد علی شاہ بھیس ڈی جی آئی ایس آئی حامد میر   \n",
       "5                        قابل اعتبار اکثر قاتل اعتبار   \n",
       "6                                انساں تھکا سوچوں سفر   \n",
       "7                                 حامد میر صاحب ویلڈن   \n",
       "8   یار وچارہ ویلا ہوندا آرے لگا ہویا ہے تسی پکے ن...   \n",
       "9                               سمجھتے پاکستان بیوقوف   \n",
       "10                                  تسی لڑاںٔی کروانی   \n",
       "\n",
       "                                         stemmed_text  \n",
       "0                             لی شاد فساد ٹھیک کوج نہ  \n",
       "1                             ہمان کھان چڑیل چاچ دسد   \n",
       "2   کامرا خا د بھریہ زمہ دار لگائ اپوزیش کر کش اور...  \n",
       "4                 راد عل شاہ بھیس ڈ ج ئ ایس ئ حامد یر  \n",
       "5                        قابل اعتبار اکثر قاتل اعتبار  \n",
       "6                                    انساں ھک سوچ سفر  \n",
       "7                                   حامد یر صاحب ویلڈ  \n",
       "8   یار وچارہ ویل ہوند رے لگ ہوی ہے س پ نجوم اس نن...  \n",
       "9                                  سمجھ پاکستا بیوقوف  \n",
       "10                                      س لڑاںٔ کروان  "
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Urdu prefixes and suffixes\n",
    "urdu_prefixes = [\n",
    "    'بے', 'غیر', 'نا', 'م', 'پھر', 'غلط', \n",
    "    'آ', 'ت', 'چھوٹا', 'بہت', 'بر', 'سب', \n",
    "    'کچھ', 'خود'\n",
    "]\n",
    "\n",
    "urdu_suffixes = [\n",
    "    'یں', 'وں', 'ی', 'ا', 'تے', 'تا', \n",
    "    'تی', 'گے', 'گی', 'کی', 'کے', 'کو', \n",
    "    'کا', 'نے', 'ہوں', 'ہیں', 'گئیں', \n",
    "    'رہا', 'رہی', 'رہے', 'یوں', 'دار', \n",
    "    'و', 'تاں', 'ائ', 'ن', 'یی'\n",
    "]\n",
    "\n",
    "def stem_urdu_word(word):\n",
    "    for prefix in urdu_prefixes:\n",
    "        if word.startswith(prefix):\n",
    "            word = word[len(prefix):]  # Remove the prefix\n",
    "\n",
    "    # Remove suffixes\n",
    "    for suffix in urdu_suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]  # Remove the suffix\n",
    "    return word  # Return the original word if no prefix or suffix is found\n",
    "\n",
    "def stem_urdu_text(text):\n",
    "    #Applies the stemming function to each word in the Urdu text.\n",
    "    if isinstance(text, str):\n",
    "        words = text.split()\n",
    "        stemmed_words = [stem_urdu_word(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "    return ''  \n",
    "\n",
    "df['stemmed_text'] = df['Emoji_Punctuation_url_removal_cleaned_text'].apply(stem_urdu_text)\n",
    "\n",
    "# Display the first 10 lines of Stemmed Txt\n",
    "df[['Emoji_Punctuation_url_removal_cleaned_text', 'stemmed_text']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>Emoji_Punctuation_url_removal_cleaned_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>Lemma_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...</td>\n",
       "      <td>لینے شادی فسادن ٹھیک کوجی نہیں</td>\n",
       "      <td>لی شاد فساد ٹھیک کوج نہ</td>\n",
       "      <td>لینے شادی فسادن ٹھیک کوجی نہیں</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "      <td>مہمانوں کھانا چڑیل چاچی دسدی میں</td>\n",
       "      <td>ہمان کھان چڑیل چاچ دسد</td>\n",
       "      <td>مہمانوں کھانا چڑیل چاچی دسدی میں</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "      <td>کامران خان دن بھریہ زمہ داری لگائی اپوزیشن کرد...</td>\n",
       "      <td>کامرا خا د بھریہ زمہ دار لگائ اپوزیش کر کش اور...</td>\n",
       "      <td>کامران خان دن بھریہ زمہ داری لگائی اپوزیشن کرد...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...</td>\n",
       "      <td>مراد علی شاہ بھیس ڈی جی آئی ایس آئی حامد میر</td>\n",
       "      <td>راد عل شاہ بھیس ڈ ج ئ ایس ئ حامد یر</td>\n",
       "      <td>مراد علی شاہ بھیس ڈی جی آئی ایس آئی حامد میر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥</td>\n",
       "      <td>قابل اعتبار اکثر قاتل اعتبار</td>\n",
       "      <td>قابل اعتبار اکثر قاتل اعتبار</td>\n",
       "      <td>قابل اعتبار اکثر قاتل اعتبار</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀</td>\n",
       "      <td>انساں تھکا سوچوں سفر</td>\n",
       "      <td>انساں ھک سوچ سفر</td>\n",
       "      <td>انساں تھکا سوچوں سفر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>حامد میر صاحب ویلڈن👏😊</td>\n",
       "      <td>حامد میر صاحب ویلڈن</td>\n",
       "      <td>حامد یر صاحب ویلڈ</td>\n",
       "      <td>حامد میر صاحب ویلڈن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 ت...</td>\n",
       "      <td>یار وچارہ ویلا ہوندا آرے لگا ہویا ہے تسی پکے ن...</td>\n",
       "      <td>یار وچارہ ویل ہوند رے لگ ہوی ہے س پ نجوم اس نن...</td>\n",
       "      <td>یار وچارہ ویلا ہوندا آرے لگا ہویا ہے تسی پکے ن...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂</td>\n",
       "      <td>سمجھتے پاکستان بیوقوف</td>\n",
       "      <td>سمجھ پاکستا بیوقوف</td>\n",
       "      <td>سمجھتے پاکستان بیوقوف</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>تسی لڑاںٔی کروانی ساڈی کی 😂😂😂</td>\n",
       "      <td>تسی لڑاںٔی کروانی</td>\n",
       "      <td>س لڑاںٔ کروان</td>\n",
       "      <td>تسی لڑاںٔی کروانی</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            urdu_text  \\\n",
       "0   🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...   \n",
       "1   چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...   \n",
       "2   کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...   \n",
       "4    `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...   \n",
       "5         قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥   \n",
       "6       انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀   \n",
       "7                               حامد میر صاحب ویلڈن👏😊   \n",
       "8   یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 ت...   \n",
       "9            یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂   \n",
       "10                      تسی لڑاںٔی کروانی ساڈی کی 😂😂😂   \n",
       "\n",
       "           Emoji_Punctuation_url_removal_cleaned_text  \\\n",
       "0                      لینے شادی فسادن ٹھیک کوجی نہیں   \n",
       "1                    مہمانوں کھانا چڑیل چاچی دسدی میں   \n",
       "2   کامران خان دن بھریہ زمہ داری لگائی اپوزیشن کرد...   \n",
       "4        مراد علی شاہ بھیس ڈی جی آئی ایس آئی حامد میر   \n",
       "5                        قابل اعتبار اکثر قاتل اعتبار   \n",
       "6                                انساں تھکا سوچوں سفر   \n",
       "7                                 حامد میر صاحب ویلڈن   \n",
       "8   یار وچارہ ویلا ہوندا آرے لگا ہویا ہے تسی پکے ن...   \n",
       "9                               سمجھتے پاکستان بیوقوف   \n",
       "10                                  تسی لڑاںٔی کروانی   \n",
       "\n",
       "                                         stemmed_text  \\\n",
       "0                             لی شاد فساد ٹھیک کوج نہ   \n",
       "1                             ہمان کھان چڑیل چاچ دسد    \n",
       "2   کامرا خا د بھریہ زمہ دار لگائ اپوزیش کر کش اور...   \n",
       "4                 راد عل شاہ بھیس ڈ ج ئ ایس ئ حامد یر   \n",
       "5                        قابل اعتبار اکثر قاتل اعتبار   \n",
       "6                                    انساں ھک سوچ سفر   \n",
       "7                                   حامد یر صاحب ویلڈ   \n",
       "8   یار وچارہ ویل ہوند رے لگ ہوی ہے س پ نجوم اس نن...   \n",
       "9                                  سمجھ پاکستا بیوقوف   \n",
       "10                                      س لڑاںٔ کروان   \n",
       "\n",
       "                                           Lemma_text  \n",
       "0                      لینے شادی فسادن ٹھیک کوجی نہیں  \n",
       "1                    مہمانوں کھانا چڑیل چاچی دسدی میں  \n",
       "2   کامران خان دن بھریہ زمہ داری لگائی اپوزیشن کرد...  \n",
       "4        مراد علی شاہ بھیس ڈی جی آئی ایس آئی حامد میر  \n",
       "5                        قابل اعتبار اکثر قاتل اعتبار  \n",
       "6                                انساں تھکا سوچوں سفر  \n",
       "7                                 حامد میر صاحب ویلڈن  \n",
       "8   یار وچارہ ویلا ہوندا آرے لگا ہویا ہے تسی پکے ن...  \n",
       "9                               سمجھتے پاکستان بیوقوف  \n",
       "10                                  تسی لڑاںٔی کروانی  "
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Urdu lemmatization dictionary\n",
    "urdu_lemmatization_dict = {\n",
    "    'چل رہی': 'چل', 'چل رہے': 'چل', 'چلیں': 'چل', 'چلو': 'چل',\n",
    "    'اچھا': 'اچھا', 'اچھی': 'اچھا', 'اچھے': 'اچھا',\n",
    "    'کرےگا': 'کرنا', 'کرےگی': 'کرنا', 'کرنا': 'کرنا', 'کر': 'کرنا',\n",
    "    'پہنچنا': 'پہنچ', 'پہنچ رہی': 'پہنچ', 'پہنچ رہے': 'پہنچ',\n",
    "    'کتابیں': 'کتاب', 'کتاب': 'کتاب', 'کتابوں': 'کتاب',\n",
    "    'محبت': 'محبت', 'محبتیں': 'محبت',\n",
    "    'خوش': 'خوش', 'خوشی': 'خوشی', 'خوشیاں': 'خوشی',\n",
    "    'بہترین': 'بہترین', 'بہتریں': 'بہترین',\n",
    "    'دیکھنا': 'دیکھ', 'دیکھ رہی': 'دیکھ', 'دیکھ رہے': 'دیکھ',\n",
    "    'ہونا': 'ہو', 'ہو رہا': 'ہو', 'ہو رہی': 'ہو', 'ہو رہے': 'ہو',\n",
    "    'لکھا': 'لکھ', 'لکھا گیا': 'لکھ', 'لکھ رہی': 'لکھ', 'لکھتے': 'لکھ',\n",
    "    'چاہتا': 'چاہنا', 'چاہتی': 'چاہنا', 'چاہیں': 'چاہنا',\n",
    "    'خریدی': 'خرید', 'خریدیں': 'خرید',\n",
    "    'دینا': 'دو', 'دی': 'دو', 'دیں': 'دو',\n",
    "    'گیا': 'جا', 'گئی': 'جا', 'گئے': 'جا',\n",
    "    'خوشبو': 'خوشبو', 'خوشبوؤں': 'خوشبو',\n",
    "    'سکھنا': 'سکھ', 'سکھانے': 'سکھ', 'سکھاتی': 'سکھ',\n",
    "    'پیار': 'پیار', 'پیاریں': 'پیار',\n",
    "    'کیا': 'کرنا', 'کرتی': 'کرنا', 'کرتے': 'کرنا', 'کروں': 'کرنا',\n",
    "    'چڑیا': 'چڑیا', 'چڑیاں': 'چڑیا',\n",
    "    'پناہ': 'پناہ', 'پناہیں': 'پناہ',\n",
    "    'چین': 'چین', 'چینیں': 'چین',\n",
    "    'جنگل': 'جنگل', 'جنگلات': 'جنگل',\n",
    "    'چاہت': 'چاہت', 'چاہتیں': 'چاہت',\n",
    "    'خواب': 'خواب', 'خوابوں': 'خواب',\n",
    "    'درد': 'درد', 'دردوں': 'درد',\n",
    "    'پہچاننا': 'پہچان', 'پہچانیں': 'پہچان',\n",
    "    'کھانا': 'کھا', 'کھا رہی': 'کھا', 'کھا رہے': 'کھا',\n",
    "    'ہنسنا': 'ہنسی', 'ہنس رہی': 'ہنسی', 'ہنس رہے': 'ہنسی',\n",
    "    'سکون': 'سکون', 'سکونت': 'سکون', 'سکونتیں': 'سکون',\n",
    "    'سچ': 'سچ', 'سچی': 'سچ', 'سچے': 'سچ',\n",
    "    'دلچسپ': 'دلچسپ', 'دلچسپیاں': 'دلچسپ',\n",
    "    'عشق': 'عشق', 'عشقیں': 'عشق',\n",
    "    'نیا': 'نیا', 'نئی': 'نیا', 'نئے': 'نیا',\n",
    "    'پرانی': 'پرانا', 'پرانا': 'پرانا',\n",
    "    'پھول': 'پھول', 'پھولوں': 'پھول',\n",
    "    'درخت': 'درخت', 'درختوں': 'درخت',\n",
    "    'انسان': 'انسان', 'انسانوں': 'انسان',\n",
    "    'بچوں': 'بچہ', 'بچہ': 'بچہ',\n",
    "    'کتوں': 'کتا', 'کتا': 'کتا',\n",
    "    'دوست': 'دوست', 'دوستوں': 'دوست',\n",
    "    'خاندان': 'خاندان', 'خاندانوں': 'خاندان',\n",
    "    'ماں': 'ماں', 'مائیں': 'ماں',\n",
    "    'باپ': 'باپ', 'باپوں': 'باپ',\n",
    "    'رشتہ': 'رشتہ', 'رشتے': 'رشتہ',\n",
    "    'خود': 'خود',\n",
    "    'زمین': 'زمین', 'زمینیں': 'زمین',\n",
    "    'مکان': 'مکان', 'مکانوں': 'مکان',\n",
    "    'دروازہ': 'دروازہ', 'دروازے': 'دروازہ',\n",
    "    'کھڑکی': 'کھڑکی', 'کھڑکیاں': 'کھڑکی',\n",
    "    'گھر': 'گھر', 'گھروں': 'گھر',\n",
    "    'کمرہ': 'کمرہ', 'کمروں': 'کمرہ',\n",
    "    'دن': 'دن', 'دنوں': 'دن',\n",
    "    'رات': 'رات', 'راتوں': 'رات',\n",
    "    'وقت': 'وقت', 'اوقات': 'وقت',\n",
    "    'زندگی': 'زندگی', 'زندگیاں': 'زندگی',\n",
    "    'موت': 'موت', 'اموات': 'موت',\n",
    "    'پانی': 'پانی', 'پانیوں': 'پانی',\n",
    "    'چائے': 'چائے', 'چائے': 'چائے',\n",
    "    'کھانا': 'کھانا', 'کھانے': 'کھانا'\n",
    "}\n",
    "\n",
    "def lemmatize_urdu_word(word):\n",
    "    #Lemmatizes Urdu words using a predefined dictionary.\n",
    "    return urdu_lemmatization_dict.get(word, word)  # Return the base form if found, else original\n",
    "\n",
    "def lemmatize_urdu_text(text):\n",
    "    #Applies the lemmatization function to each word in the Urdu text.\n",
    "    if isinstance(text, str):\n",
    "        words = text.split()\n",
    "        lemmatized_words = [lemmatize_urdu_word(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "    return ''\n",
    "# Apply lemmatization\n",
    "df['Lemma_text'] = df['Emoji_Punctuation_url_removal_cleaned_text'].apply(lemmatize_urdu_text)\n",
    "\n",
    "df[['urdu_text','Emoji_Punctuation_url_removal_cleaned_text','stemmed_text','Lemma_text']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemma_text</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>لینے شادی فسادن ٹھیک کوجی نہیں</td>\n",
       "      <td>[لینے, شادی, فسادن, ٹھیک, کوجی, نہیں]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>مہمانوں کھانا چڑیل چاچی دسدی میں</td>\n",
       "      <td>[مہمانوں, کھانا, چڑیل, چاچی, دسدی, میں]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان دن بھریہ زمہ داری لگائی اپوزیشن کرد...</td>\n",
       "      <td>[کامران, خان, دن, بھریہ, زمہ, داری, لگائی, اپو...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مراد علی شاہ بھیس ڈی جی آئی ایس آئی حامد میر</td>\n",
       "      <td>[مراد, علی, شاہ, بھیس, ڈی, جی, آئی, ایس, آئی, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>قابل اعتبار اکثر قاتل اعتبار</td>\n",
       "      <td>[قابل, اعتبار, اکثر, قاتل, اعتبار]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>انساں تھکا سوچوں سفر</td>\n",
       "      <td>[انساں, تھکا, سوچوں, سفر]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>حامد میر صاحب ویلڈن</td>\n",
       "      <td>[حامد, میر, صاحب, ویلڈن]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>یار وچارہ ویلا ہوندا آرے لگا ہویا ہے تسی پکے ن...</td>\n",
       "      <td>[یار, وچارہ, ویلا, ہوندا, آرے, لگا, ہویا, ہے, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>سمجھتے پاکستان بیوقوف</td>\n",
       "      <td>[سمجھتے, پاکستان, بیوقوف]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>تسی لڑاںٔی کروانی</td>\n",
       "      <td>[تسی, لڑاںٔی, کروانی]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Lemma_text  \\\n",
       "0                      لینے شادی فسادن ٹھیک کوجی نہیں   \n",
       "1                    مہمانوں کھانا چڑیل چاچی دسدی میں   \n",
       "2   کامران خان دن بھریہ زمہ داری لگائی اپوزیشن کرد...   \n",
       "4        مراد علی شاہ بھیس ڈی جی آئی ایس آئی حامد میر   \n",
       "5                        قابل اعتبار اکثر قاتل اعتبار   \n",
       "6                                انساں تھکا سوچوں سفر   \n",
       "7                                 حامد میر صاحب ویلڈن   \n",
       "8   یار وچارہ ویلا ہوندا آرے لگا ہویا ہے تسی پکے ن...   \n",
       "9                               سمجھتے پاکستان بیوقوف   \n",
       "10                                  تسی لڑاںٔی کروانی   \n",
       "\n",
       "                                           token_text  \n",
       "0               [لینے, شادی, فسادن, ٹھیک, کوجی, نہیں]  \n",
       "1             [مہمانوں, کھانا, چڑیل, چاچی, دسدی, میں]  \n",
       "2   [کامران, خان, دن, بھریہ, زمہ, داری, لگائی, اپو...  \n",
       "4   [مراد, علی, شاہ, بھیس, ڈی, جی, آئی, ایس, آئی, ...  \n",
       "5                  [قابل, اعتبار, اکثر, قاتل, اعتبار]  \n",
       "6                           [انساں, تھکا, سوچوں, سفر]  \n",
       "7                            [حامد, میر, صاحب, ویلڈن]  \n",
       "8   [یار, وچارہ, ویلا, ہوندا, آرے, لگا, ہویا, ہے, ...  \n",
       "9                           [سمجھتے, پاکستان, بیوقوف]  \n",
       "10                              [تسی, لڑاںٔی, کروانی]  "
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load the multilingual model\n",
    "nlp = spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "# Tokenization function using SpaCy\n",
    "def tokenize_urdu(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "# Appply Tokenized\n",
    "df['token_text'] = df['Lemma_text'].apply(tokenize_urdu)\n",
    "df[['Lemma_text','token_text']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6302</th>\n",
       "      <td>تو</td>\n",
       "      <td>499.924465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>دو</td>\n",
       "      <td>243.606889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15592</th>\n",
       "      <td>نہ</td>\n",
       "      <td>198.743859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2410</th>\n",
       "      <td>اب</td>\n",
       "      <td>194.253786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19734</th>\n",
       "      <td>کوئی</td>\n",
       "      <td>182.443432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3194</th>\n",
       "      <td>اللہ</td>\n",
       "      <td>179.840199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7703</th>\n",
       "      <td>خان</td>\n",
       "      <td>175.210094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7271</th>\n",
       "      <td>جی</td>\n",
       "      <td>166.572750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4240</th>\n",
       "      <td>بات</td>\n",
       "      <td>165.799989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19457</th>\n",
       "      <td>کریں</td>\n",
       "      <td>158.604370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  TF-IDF Score\n",
       "6302     تو    499.924465\n",
       "8291     دو    243.606889\n",
       "15592    نہ    198.743859\n",
       "2410     اب    194.253786\n",
       "19734  کوئی    182.443432\n",
       "3194   اللہ    179.840199\n",
       "7703    خان    175.210094\n",
       "7271     جی    166.572750\n",
       "4240    بات    165.799989\n",
       "19457  کریں    158.604370"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load Urdu language model for spaCy\n",
    "nlp = spacy.blank(\"ur\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_urdu(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc if token.text not in urdu_stopwords]\n",
    "# Tokenizing the 'urdu_text' column\n",
    "df['TF_IDF_token_text'] = df['Lemma_text'].apply(tokenize_urdu)\n",
    "\n",
    "# Joining the tokenized lists back into strings for TF-IDF processing\n",
    "df['TF_IDF_token_text'] = df['TF_IDF_token_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# TF-IDF Vectorization with stop words removed\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['TF_IDF_token_text'])\n",
    "\n",
    "# Getting the TF-IDF scores\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame\n",
    "tfidf_df = pd.DataFrame({'Word': words, 'TF-IDF Score': tfidf_scores})\n",
    "tfidf_df = tfidf_df.sort_values(by='TF-IDF Score', ascending=False)\n",
    "\n",
    "# Display the top 10 words with the highest TF-IDF scores\n",
    "tfidf_df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Words Most Similar to 'اچھا' (Good):\n",
      "ہوتی: 0.989397406578064\n",
      "تکلیف: 0.9884074330329895\n",
      "دل: 0.9862785339355469\n",
      "محبت: 0.9856767058372498\n",
      "قیمتی: 0.9847457408905029\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# Train Word2Vec model on tokenized Urdu data\n",
    "#Take Word2Vec list of tokenized sentences\n",
    "sentences = df['token_text'].tolist()\n",
    "\n",
    "# Train the model\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Find top 5 words most similar to 'اچھا' (\"good\")\n",
    "similar_words = word2vec_model.wv.most_similar(\"اچھا\", topn=5)\n",
    "\n",
    "# Display most similar words\n",
    "print(\"Top 5 Words Most Similar to 'اچھا' (Good):\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Common Bigrams with Frequencies:\n",
      "عمران خان: 500\n",
      "؟ ؟: 479\n",
      "نواز شریف: 446\n",
      "آئی جی: 312\n",
      "سندھ پولیس: 299\n",
      "آرمی چیف: 223\n",
      "خان صاحب: 181\n",
      "کیپٹن صفدر: 177\n",
      "🇵 🇰: 162\n",
      "جزاک اللہ: 158\n",
      "\n",
      "Top 10 Most Common Trigrams with Frequencies:\n",
      "؟ ؟ ؟: 226\n",
      "آئی جی سندھ: 118\n",
      "پی ٹی آئی: 114\n",
      "پی ڈی ایم: 86\n",
      "صلی اللہ علیہ: 86\n",
      "فالو کریں فالو: 74\n",
      "جزاک اللہ خیر: 71\n",
      "کریں فالو بیک: 71\n",
      "آئی جی اغوا: 71\n",
      "والوں فالو کریں: 69\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "# Load Urdu tokenizer using spaCy \n",
    "nlp = spacy.blank(\"ur\")\n",
    "\n",
    "#Function to generate n-grams\n",
    "def get_ngrams(tokenized_text, n):\n",
    "    return list(ngrams(tokenized_text, n))\n",
    "\n",
    "# Generate unigrams, bigrams, and trigrams from the tokenized text\n",
    "df['unigrams'] = df['token_text'].apply(lambda x: get_ngrams(x, 1))\n",
    "df['bigrams'] = df['token_text'].apply(lambda x: get_ngrams(x, 2))\n",
    "df['trigrams'] = df['token_text'].apply(lambda x: get_ngrams(x, 3))\n",
    "\n",
    "#Flatten the list of bigrams and trigrams for frequency counting\n",
    "all_bigrams = [bigram for sublist in df['bigrams'] for bigram in sublist]\n",
    "all_trigrams = [trigram for sublist in df['trigrams'] for trigram in sublist]\n",
    "\n",
    "#Count the frequency of bigrams and trigrams\n",
    "bigram_freq = Counter(all_bigrams)\n",
    "trigram_freq = Counter(all_trigrams)\n",
    "\n",
    "#Get the top 10 most common bigrams and trigrams\n",
    "top_10_bigrams = bigram_freq.most_common(10)\n",
    "top_10_trigrams = trigram_freq.most_common(10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 Most Common Bigrams with Frequencies:\")\n",
    "for bigram, freq in top_10_bigrams:\n",
    "    print(f\"{' '.join(bigram)}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 10 Most Common Trigrams with Frequencies:\")\n",
    "for trigram, freq in top_10_trigrams:\n",
    "    print(f\"{' '.join(trigram)}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Metrics:\n",
      "Accuracy: 0.7797\n",
      "Precision: 0.7802\n",
      "Recall: 0.7797\n",
      "F1-Score: 0.7794\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#Prepare the data\n",
    "X = df['TF_IDF_token_text']\n",
    "y = df['is_sarcastic']\n",
    "\n",
    "#Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Create a TF-IDF vectorizer and logistic regression model pipeline\n",
    "model = make_pipeline(TfidfVectorizer(), LogisticRegression(max_iter=1000))\n",
    "\n",
    "#Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "#Display the results\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.75      0.76      1775\n",
      "         1.0       0.78      0.81      0.80      1979\n",
      "\n",
      "    accuracy                           0.78      3754\n",
      "   macro avg       0.78      0.78      0.78      3754\n",
      "weighted avg       0.78      0.78      0.78      3754\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression for Sentiment Classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, df['is_sarcastic'], test_size=0.2)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.75      0.76      1775\n",
      "         1.0       0.78      0.81      0.80      1979\n",
      "\n",
      "    accuracy                           0.78      3754\n",
      "   macro avg       0.78      0.78      0.78      3754\n",
      "weighted avg       0.78      0.78      0.78      3754\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation Metrics\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the model performance using test set predictions\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified texts:\n",
      "  (0, 8763)\t0.43961025138992377\n",
      "  (0, 20810)\t0.47452896039611114\n",
      "  (0, 5325)\t0.2693137041028108\n",
      "  (0, 13138)\t0.40842531485148803\n",
      "  (0, 16956)\t0.320427025847638\n",
      "  (0, 19137)\t0.3538912232974851\n",
      "  (0, 16233)\t0.3076619447443641\n",
      "  (0, 6302)\t0.14019855393235375\n",
      "  (1, 16300)\t0.2697738438201112\n",
      "  (1, 4506)\t0.2610282206655509\n",
      "  (1, 5754)\t0.27832713125679903\n",
      "  (1, 5315)\t0.29498046016561186\n",
      "  (1, 11048)\t0.5052455659180036\n",
      "  (1, 17707)\t0.2899550231999509\n",
      "  (1, 11947)\t0.20706008976687007\n",
      "  (1, 7437)\t0.21013066923355878\n",
      "  (1, 4256)\t0.21781031387708094\n",
      "  (1, 18788)\t0.2526227829590018\n",
      "  (1, 20753)\t0.18733288896228817\n",
      "  (1, 12972)\t0.17418113474920308\n",
      "  (1, 16354)\t0.16810396755622672\n",
      "  (1, 8652)\t0.160608283837478\n",
      "  (1, 8291)\t0.11252779955310588\n",
      "  (1, 6302)\t0.08324596379893331\n",
      "  (1, 11239)\t0.13376269229850715\n",
      "  :\t:\n",
      "  (823, 6826)\t0.16938233303491282\n",
      "  (824, 8896)\t0.36314900338650125\n",
      "  (824, 12504)\t0.3135345392808945\n",
      "  (824, 4383)\t0.2742160067305132\n",
      "  (824, 18429)\t0.2559339302211\n",
      "  (824, 14771)\t0.2700006377881608\n",
      "  (824, 161)\t0.26744317358506625\n",
      "  (824, 17815)\t0.2445367034451695\n",
      "  (824, 9452)\t0.25970470623293535\n",
      "  (824, 12030)\t0.24654410454836145\n",
      "  (824, 6682)\t0.18654778022521615\n",
      "  (824, 9717)\t0.1976245995732084\n",
      "  (824, 13110)\t0.2559339302211\n",
      "  (824, 5759)\t0.2302429675729785\n",
      "  (824, 13285)\t0.2224318316260118\n",
      "  (824, 21681)\t0.1716948134510436\n",
      "  (824, 13341)\t0.15919418382694547\n",
      "  (825, 13246)\t0.2584380756174065\n",
      "  (825, 7879)\t0.24012558196971903\n",
      "  (825, 6866)\t0.45625694365178526\n",
      "  (825, 13634)\t0.3427340799571628\n",
      "  (825, 15466)\t0.6051589248596619\n",
      "  (825, 18540)\t0.2685088673796414\n",
      "  (825, 2047)\t0.2726306814649602\n",
      "  (825, 2410)\t0.19305593253115974\n"
     ]
    }
   ],
   "source": [
    "# Error Analysis\n",
    "\n",
    "#finding misclassified examples\n",
    "misclassified = X_test[(y_test != y_pred)]\n",
    "print(\"Misclassified texts:\")\n",
    "print(misclassified)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
